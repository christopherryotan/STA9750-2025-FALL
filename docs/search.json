[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Goal: Project Set-up for STA9750"
  },
  {
    "objectID": "projects.html#mini-project-00",
    "href": "projects.html#mini-project-00",
    "title": "",
    "section": "",
    "text": "Goal: Project Set-up for STA9750"
  },
  {
    "objectID": "projects.html#mini-project-01",
    "href": "projects.html#mini-project-01",
    "title": "",
    "section": "Mini-Project #1",
    "text": "Mini-Project #1\n\n\nGoal: Analyze Netflix’s Top 10 data to identify the most popular programming on the platform."
  },
  {
    "objectID": "projects.html#mini-project-02",
    "href": "projects.html#mini-project-02",
    "title": "",
    "section": "Mini-Project #2",
    "text": "Mini-Project #2\n\n\nGoal: Identify America’s most “YIMBY” cities using a variety of census data sources and real estate indices."
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "",
    "text": "For this assignment, we are given the task of analyzing Netflix’s Global Top 10 and Country Top 10 datasets. Below we will acquire, clean, and explore the data, then answer exploratory questions and draft press releases."
  },
  {
    "objectID": "mp01.html#objective",
    "href": "mp01.html#objective",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "",
    "text": "For this assignment, we are given the task of analyzing Netflix’s Global Top 10 and Country Top 10 datasets. Below we will acquire, clean, and explore the data, then answer exploratory questions and draft press releases."
  },
  {
    "objectID": "mp01.html#acquire-data",
    "href": "mp01.html#acquire-data",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Acquire Data",
    "text": "Acquire Data\n\n\nShow code\nif (!dir.exists(file.path(\"data\", \"mp01\"))) {\n  dir.create(file.path(\"data\", \"mp01\"), showWarnings = FALSE, recursive = TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME  &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif (!file.exists(GLOBAL_TOP_10_FILENAME)) {\n  download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\",\n                destfile = GLOBAL_TOP_10_FILENAME)\n}\nif (!file.exists(COUNTRY_TOP_10_FILENAME)) {\n  download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\",\n                destfile = COUNTRY_TOP_10_FILENAME)\n}"
  },
  {
    "objectID": "mp01.html#data-import",
    "href": "mp01.html#data-import",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Data Import",
    "text": "Data Import\n\n\nShow code\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\n\n\n\n\nShow code\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME)"
  },
  {
    "objectID": "mp01.html#data-cleaningreformatting",
    "href": "mp01.html#data-cleaningreformatting",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Data Cleaning/Reformatting",
    "text": "Data Cleaning/Reformatting\n\n\nShow code\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt;\nmutate(season_title = if_else(season_title == \"N/A\", NA_character_, season_title))\n\n\n\n\nShow code\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME,\n                          na = \"N/A\")"
  },
  {
    "objectID": "mp01.html#intial-data-exploration",
    "href": "mp01.html#intial-data-exploration",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Intial Data Exploration",
    "text": "Intial Data Exploration\n\n\nShow code\nlibrary(DT)\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\n\n\nShow code\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\nShow code\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\nShow code\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))"
  },
  {
    "objectID": "mp01.html#exploratory-questions",
    "href": "mp01.html#exploratory-questions",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Exploratory Questions",
    "text": "Exploratory Questions\n\n1. How many different countries does Netflix operate in?\n\n\nShow code\ncountries &lt;- COUNTRY_TOP_10 |&gt; \n  summarise(num_countries = n_distinct(country_name))\n\n\nNetflix operates in 94 countries.\n\n\n2. Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\nShow code\nnonEnglish &lt;- GLOBAL_TOP_10 |&gt; \n  filter(category == \"Films (Non-English)\") |&gt;\n  arrange(desc(cumulative_weeks_in_top_10)) |&gt;\n  slice(1) |&gt;\n  select(show_title, cumulative_weeks_in_top_10)\n\n\nThe non-English film with the most cumulative weeks in the global Top 10 is All Quiet on the Western Front, with 23 weeks.\n\n\n3. What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\nShow code\nlongest &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(category, \"Films\")) |&gt;\n  mutate(runtime_minutes = runtime * 60) |&gt;\n  arrange(desc(runtime_minutes)) |&gt; \n  slice(1) |&gt;\n  select(show_title, runtime_minutes)\n\n\nThe longest film to have ever appeared in the Netflix global Top 10 was Pushpa 2: The Rule (Reloaded Version) with 224 minutes.\n\n\n4. For each of the four categories, what program has the most total hours of global viewership?\n\n\nShow code\ntop_by_category &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop_last\") |&gt;\n  slice_max(order_by = total_hours, n = 1, with_ties = FALSE) |&gt;\n  ungroup()\n\n\nThe programs with the most total hours of global viewership are KPop Demon Hunters for English films, Society of the Snow for non-English films, Stranger Things for English TV, and Squid Game for non-English TV.\n\n\n5. Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\nShow code\nlongest_tv_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(category == \"TV\") |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarise(longest_run = max(cumulative_weeks_in_top_10, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(longest_run)) |&gt;\n  slice(1)   \n\n\nThe TV show that had the longest run in a country’s Top 10 is Money Heist, which appeared in Pakistan for 127 weeks.\n\n\n6. Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\nShow code\ndropped_country &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(max_week = max(week, na.rm = TRUE)) |&gt;\n  arrange(max_week) |&gt;\n  slice(1)\n\n\nThe country where Netflix ceased operations the earliest is Russia, in the week of 2022-02-27.\n\n\n7. What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\nShow code\nsquid_game_viewership &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(show_title, \"Squid Game\")) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\nThe total viewership of the TV show Squid Game across all seasons is 5,310,000,000 hours.\n\n\n8. The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021?\n\n\nShow code\nred_notice_views_2021 &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", year(week) == 2021) |&gt;\n  summarise(total_views = sum(weekly_hours_viewed, na.rm = TRUE) / ((1 + 58 / 60))) |&gt;\n  mutate(total_views = round(total_views))\n\n\nThe movie Red Notice received approximately 201,732,203 views in 2021.\n\n\n9. How many Films reached Number 1 in the US but did not originally debut there?\n\n\nShow code\nlibrary(dplyr)\nlibrary(stringr)\n\nus_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"United States\",\n         str_starts(category, \"Films\")) |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    debut_week = min(week, na.rm = TRUE),\n    debut_rank = weekly_rank[which.min(week)],\n    hit_one = any(weekly_rank == 1, na.rm = TRUE),\n    last_week_one = if (any(weekly_rank == 1, na.rm = TRUE))\n      max(week[weekly_rank == 1], na.rm = TRUE)\n    else as.Date(NA),\n    .groups = \"drop\"\n  )\n\nclimbers &lt;- us_films |&gt;\n  filter(hit_one, debut_rank &gt; 1) |&gt;\n  arrange(desc(last_week_one))\n\nfilms_count &lt;- nrow(climbers)\nlatest_row  &lt;- slice_head(climbers, n = 1)\nlatest_film &lt;- latest_row$show_title\nlatest_when &lt;- latest_row$last_week_one\n\n\nA total of 45 films reached Number 1 in the United States after debuting below the top spot.\nThe most recent film to do this was KPop Demon Hunters, which reached Number 1 on 2025-09-14.\n\n\n10. Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\nShow code\nlibrary(dplyr)\nlibrary(stringr)\n\ntv_debut_spread &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_starts(category, \"TV\")) |&gt;\n  group_by(show_title, season_title) |&gt;\n  mutate(debut_week = min(week, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  filter(week == debut_week) |&gt;\n  group_by(show_title, season_title) |&gt;\n  summarise(\n    countries = n_distinct(country_name),\n    debut_week = first(debut_week),\n    .groups = \"drop\"\n  ) |&gt;\n  slice_max(countries, n = 1, with_ties = FALSE)\n\ntop_title     &lt;- tv_debut_spread$show_title\ntop_season    &lt;- tv_debut_spread$season_title\ntop_countries &lt;- tv_debut_spread$countries\ntop_week      &lt;- tv_debut_spread$debut_week\n\n\nThe TV show/season that hit the Top 10 in the most countries in its debut week was Emily in Paris — Emily in Paris: Season 2, charting in 94 countries during the week of 2021-12-26."
  },
  {
    "objectID": "mp01.html#press-releases",
    "href": "mp01.html#press-releases",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Press Releases",
    "text": "Press Releases\n\n\nShow code\n# Global table uses \"TV (English)\" for the category.\nst_global &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (English)\",\n         str_detect(show_title, regex(\"^Stranger Things$\", ignore_case = TRUE)))\n\nst_total_hours &lt;- sum(st_global$weekly_hours_viewed, na.rm = TRUE)  # FACT 1\nst_total_weeks &lt;- nrow(st_global)                                   # FACT 2\n\n# Country table uses \"TV\" (no language tag). Count unique countries.\nst_countries &lt;- COUNTRY_TOP_10 |&gt;\n  filter(category == \"TV\",\n         str_detect(show_title, regex(\"^Stranger Things$\", ignore_case = TRUE)))\n\nst_num_countries &lt;- st_countries |&gt;\n  distinct(country_name) |&gt;\n  nrow()                                                            # FACT 3\n\n# Compare against other English-language TV series by total hours viewed\ntv_eng_totals &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(total_hours))\n\nst_rank &lt;- tv_eng_totals |&gt;\n  mutate(rank = dense_rank(desc(total_hours))) |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  pull(rank)\n\nsecond_place  &lt;- tv_eng_totals |&gt; slice(2)\nsecond_show   &lt;- second_place$show_title\nsecond_hours  &lt;- second_place$total_hours\n\n\n\nStranger Things Season 5: The Resurgence of a Legacy\nNetflix’s Stranger Things remains a once-in-a-generation hit as many await its final release in November 2025. Across its first four seasons, the series has amassed 2,967,980,000 hours viewed globally, spent 50 weeks in the Global Top 10, and charted in 93 countries. Among English-language TV series by total hours viewed, it ranks #1, ahead of hits like Wednesday (2,842,150,000 hours), underscoring extraordinary staying power and worldwide appeal ahead of the final season.\n\n\nShow code\n# Identify Hindi-language titles \nhindi_global &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category %in% c(\"Films (Non-English)\", \"TV (Non-English)\"),\n         str_detect(show_title, regex(\"Hindi|India\", ignore_case = TRUE)))\n\n# Aggregate total hours viewed globally \nhindi_total_hours &lt;- sum(hindi_global$weekly_hours_viewed, na.rm = TRUE)\n\n# Find top Hindi titles by global hours viewed\ntop_hindi &lt;- hindi_global |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(total_hours)) |&gt;\n  slice_head(n = 3)\n\ntop_hindi_1  &lt;- top_hindi$show_title[1]\ntop_hindi_1h &lt;- top_hindi$total_hours[1]\ntop_hindi_2  &lt;- top_hindi$show_title[2]\ntop_hindi_2h &lt;- top_hindi$total_hours[2]\ntop_hindi_3  &lt;- top_hindi$show_title[3]\ntop_hindi_3h &lt;- top_hindi$total_hours[3]\n\n# Estimate the number of unique Hindi titles appearing in the dataset\nhindi_title_count &lt;- n_distinct(hindi_global$show_title)\n\n# Estimate number of Hindi entries (viewing records)\nhindi_records &lt;- nrow(hindi_global)\n\n\n\n\nNetflix Celebrates Blockbuster Growth in India\nIndia’s love for local storytelling continues to fuel Netflix’s remarkable rise in South Asia. Over the past two years, Hindi-language films and series have generated more than 244,100,000 hours viewed worldwide. Among these titles, hits like RRR (Hindi) (79,780,000 hours), The Great Indian Kapil Show (44,600,000 hours), and Kalki 2898 AD (Hindi) (23,100,000 hours) have captivated audiences at home as well as overseas. With over 20 Hindi-language titles appearing in the global Top 10 across 60 charting weeks, Netflix continues to deepen its investment in local content — cementing India as one of its fastest-growing creative and consumer markets.\n\n\nShow code\njp_global &lt;- GLOBAL_TOP_10 |&gt; \n  filter(category %in% c(\"Films (Non-English)\", \"TV (Non-English)\")) |&gt; \n  filter(str_detect(show_title, regex(\"Japan|Japanese|の|アニメ|ドラマ|Tokyo|Naruto|One Piece|Jujutsu Kaisen|Demon Slayer|Spirited Away|Ghibli|Boruto\", ignore_case = TRUE)))\n\n# Compute totals\njp_total_hours &lt;- sum(jp_global$weekly_hours_viewed, na.rm = TRUE)\njp_title_count &lt;- n_distinct(jp_global$show_title)\njp_records &lt;- nrow(jp_global)\n\n# Find top 3 Japanese titles by total hours viewed\ntop_jp &lt;- jp_global |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") |&gt; \n  arrange(desc(total_hours)) |&gt; \n  slice_head(n = 3)\n\ntop_jp_1  &lt;- top_jp$show_title[1]\ntop_jp_1h &lt;- top_jp$total_hours[1]\ntop_jp_2  &lt;- top_jp$show_title[2]\ntop_jp_2h &lt;- top_jp$total_hours[2]\ntop_jp_3  &lt;- top_jp$show_title[3]\ntop_jp_3h &lt;- top_jp$total_hours[3]\n\n\n\n\nThe Global Appeal of Japanese Storytelling\nJapanese films and series are finding fans in every corner of the world. From touching dramas to breathtaking anime, stories from Japan continue to draw in global audiences with their emotion, imagination, and artistry. Over the past few years, viewers have spent more than 178,970,000 hours watching Japanese titles on Netflix. Fan favorites like Demon Slayer: Kimetsu no Yaiba (89,900,000 hours), Tokyo Swindlers (41,900,000 hours), and Jujutsu Kaisen (22,400,000 hours) continue to reach new audiences and showcase the creativity of Japanese storytellers. With more than 8 Japanese titles appearing in the Top 10 across 30 charting weeks, Netflix’s collaboration with Japan’s creators is helping share their stories with millions of viewers around the world."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Hi everyone, I’m Christopher Tan! I began my undergrad planning to pursue a career in accounting, but towards the end of my senior year, I took a handful of programming classes and discovered how much I enjoyed problem solving. That led me to pursuing my Masters in Statistics, where I could learn to make sense of large amounts of data as well as understanding statistical methodologies. I’m especially interested in ML and data science, since they bring together coding and statistical analysis in meaningful ways. Looking forward to meeting you folks!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "",
    "section": "",
    "text": "Hi everyone, I’m Christopher Tan! I began my undergrad planning to pursue a career in accounting, but towards the end of my senior year, I took a handful of programming classes and discovered how much I enjoyed problem solving. That led me to pursuing my Masters in Statistics, where I could learn to make sense of large amounts of data as well as understanding statistical methodologies. I’m especially interested in ML and data science, since they bring together coding and statistical analysis in meaningful ways. Looking forward to meeting you folks!"
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "",
    "text": "For this assignment, we will identify America’s most “YIMBY” cities using a variety of census data sources and real estate indices. Below, we will use the results of our analysis to lobby politicians in support of a federal YIMBY-incentive program. We will then prepare a short policy brief that is designed to help us find congressional representatives whose districts would benefit from adopting YIMBY-type policies and whose reelection prospects would be improved by sponsoring this type of bill."
  },
  {
    "objectID": "mp02.html#objective",
    "href": "mp02.html#objective",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "",
    "text": "For this assignment, we will identify America’s most “YIMBY” cities using a variety of census data sources and real estate indices. Below, we will use the results of our analysis to lobby politicians in support of a federal YIMBY-incentive program. We will then prepare a short policy brief that is designed to help us find congressional representatives whose districts would benefit from adopting YIMBY-type policies and whose reelection prospects would be improved by sponsoring this type of bill."
  },
  {
    "objectID": "mp02.html#data-acquistion",
    "href": "mp02.html#data-acquistion",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Data Acquistion",
    "text": "Data Acquistion\n\n\nShow code\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nensure_package(tidyverse)\nensure_package(glue)\nensure_package(readxl)\nensure_package(tidycensus)\n\n\n\n\nShow code\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\n\n\nShow code\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\nensure_package(httr2)\nensure_package(rvest)\n\n\n\n\nShow code\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    \n    if(!file.exists(fname)){\n    \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code)\n    \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n    \n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\n\n\nShow code\nensure_package(httr2)\nensure_package(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\n\n\n\nShow code\nWAGES &lt;- get_bls_qcew_annual_averages()"
  },
  {
    "objectID": "mp02.html#exploratory-questions",
    "href": "mp02.html#exploratory-questions",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Exploratory Questions",
    "text": "Exploratory Questions\n\n1. Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nShow code\nlibrary(dplyr)\nlibrary(DT)\nlibrary(scales)\n\nPERMITS |&gt;\n  filter(year &gt;= 2010, year &lt;= 2019) |&gt;\n  group_by(CBSA) |&gt;\n  summarize(`New Housing Units Permitted` = sum(new_housing_units_permitted)) |&gt;\n  left_join(INCOME |&gt; distinct(GEOID, NAME), join_by(CBSA == GEOID)) |&gt;\n  arrange(desc(`New Housing Units Permitted`)) |&gt;\n  rename(\n    Name = NAME\n  ) |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\nCBSA 26420 permitted the largest number of new housing units in the decade from 2010 to 2019.\n\n\n2. In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nShow code\n# Creates summary table\nabq_summary &lt;- PERMITS |&gt;\n  filter(CBSA == 10740, year &gt;= 2010, year &lt;= 2019) |&gt;\n  group_by(year) |&gt;\n  summarise(total_units = sum(new_housing_units_permitted), .groups = \"drop\")\n\n# Preview table (pretty column names only for display)\nabq_summary |&gt;\n  arrange(desc(total_units)) |&gt;\n  rename(Year = year, `Total Units` = total_units) |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\nShow code\n# Values for inline text\nabq_top   &lt;- slice_max(abq_summary, order_by = total_units, n = 1, with_ties = FALSE)\nabq_year  &lt;- abq_top$year\nabq_units &lt;- abq_top$total_units\n\n\nAlbuquerque, NM permitted the most new housing units in 2013, with 2606 units approved.\n\n\n3. Which state (not CBSA) had the highest average individual income in 2015?\n\n\nShow code\nstate_income_2015 &lt;- INCOME |&gt;\nfilter(year == 2015) |&gt;\nleft_join(HOUSEHOLDS |&gt; filter(year == 2015), join_by(GEOID)) |&gt;\nleft_join(POPULATION  |&gt; filter(year == 2015), join_by(GEOID)) |&gt;\nmutate(\nName = str_extract(NAME.x, \", (.{2})\", group = 1),\n`Total Income (CBSA)` = household_income * households,\n`Average Individual Income` = `Total Income (CBSA)` / population\n) |&gt;\nselect(Name, `Average Individual Income`) |&gt;\nfilter(!is.na(`Average Individual Income`)) |&gt;\narrange(desc(`Average Individual Income`)) |&gt;\nmutate(`Average Individual Income` = dollar(round(`Average Individual Income`, 2)))\n\ndatatable(\nhead(state_income_2015, 10),\noptions = list(pageLength = 10, searching = FALSE, info = FALSE),\ncaption = \"Top States by Average Individual Income (2015)\"\n)\n\n\n\n\n\n\nIn 2015, CA (California) had the highest per-capita income.\n\n\n4. What is the last year in which the NYC CBSA had the most data scientists in the country?\n\n\nShow code\nt1 &lt;- INCOME |&gt; \n  mutate(std_cbsa = paste0(\"C\", GEOID))\n\nt2 &lt;- WAGES |&gt; \n  mutate(std_cbsa = paste0(FIPS, \"0\"))\n\ninner_join(t1, t2, join_by(std_cbsa == std_cbsa)) |&gt;\n  filter(INDUSTRY == 5182) |&gt;\n  group_by(YEAR, std_cbsa) |&gt;\n  summarize(`Employment Number` = sum(EMPLOYMENT)) |&gt;\n  arrange(YEAR, desc(`Employment Number`)) |&gt;\n  filter(`Employment Number` == first(`Employment Number`)) |&gt;\n  filter(std_cbsa == \"C35620\") |&gt;\n  arrange(desc(`Employment Number`)) |&gt;\n  rename(\n    Year = YEAR,\n    CBSA = std_cbsa\n  ) |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\n2015 is the last year in which the NYC CBSA had the most data scientists in the country.\n\n\n5. What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nShow code\nWAGES |&gt;\n  filter(FIPS == \"C3562\") |&gt;\n  group_by(YEAR) |&gt;\n  summarize(\n    `Total Wages` = sum(TOTAL_WAGES),\n    `Finance and Insurance Wages` = sum(ifelse(INDUSTRY == 52, TOTAL_WAGES, 0)),\n    `Share of Finance & Insurance` = `Finance and Insurance Wages` / `Total Wages`,\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    `Total Wages` = scales::dollar(`Total Wages`),\n    `Finance and Insurance Wages` = scales::dollar(`Finance and Insurance Wages`),\n    `Share of Finance & Insurance` = scales::percent(`Share of Finance & Insurance`, accuracy = 0.1)\n  ) |&gt;\n  arrange(desc(`Share of Finance & Insurance`)) |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\nThe fractions of total wages in the NYC CBSA that was earned by people employed in the finance/insurance industries peaked in 2014."
  },
  {
    "objectID": "mp02.html#visualizations",
    "href": "mp02.html#visualizations",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Visualizations",
    "text": "Visualizations\n\n1. The relationship between monthly rent and average household income per CBSA in 2009.\n\n\nShow code\nlibrary(ggplot2)\n\nQ1 &lt;- INCOME |&gt; \n  filter(year == 2009) |&gt;\n  inner_join(RENT |&gt; filter(year == 2009), join_by(GEOID == GEOID)) |&gt;\n  select(NAME.x, household_income, monthly_rent)\n\nggplot(Q1, aes(x = household_income, y = monthly_rent)) +\n  geom_point(alpha = 0.35, size = 2.8, color = \"steelblue4\") +\n  \n  stat_smooth(se = FALSE, color = \"red4\", linewidth = 1.2) +\n  \n  scale_y_continuous(labels = scales::dollar) +\n  scale_x_continuous(labels = scales::dollar) +\n  \n  labs(\n    title = \"Relationship Between Monthly Rent and Average Household Income (2009)\",\n    subtitle = \"Each point represents a U.S. CBSA (Core-Based Statistical Area)\",\n    x = \"Average Household Income (USD)\",\n    y = \"Average Monthly Rent (USD)\"\n  ) +\n  \n  theme_bw(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15),\n    plot.subtitle = element_text(size = 12, color = \"gray30\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"gray90\"),\n    axis.title = element_text(face = \"bold\"),\n    plot.margin = margin(10, 15, 10, 15)\n  )\n\n\n\n\n\n\n\n\n\n\n\n2. The relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs.\n\n\nShow code\n# Get total employment by CBSA and year \n#| fig-width: 15 #set dimensions of figure\n#| fig-height: 15\n#sum wages by fips and year\ntotal_employment &lt;- WAGES |&gt;\n  group_by(FIPS, YEAR) |&gt; \n  summarize(total_emp = sum(EMPLOYMENT, na.rm = TRUE), .groups = 'drop')\n# Get healthcare employment, NAICS 62. sum by fips and year\nhealthcare_employment &lt;- WAGES |&gt;\n  filter(str_starts(as.character(INDUSTRY), \"62\")) |&gt;\n  group_by(FIPS, YEAR) |&gt;\n  summarize(healthcare_emp = sum(EMPLOYMENT, na.rm = TRUE), .groups = 'drop')\n# inner join and filter where employment is greater than 0 \nemployment_data &lt;- total_employment |&gt;\n  inner_join(healthcare_employment, by = c(\"FIPS\", \"YEAR\")) |&gt;\n  filter(total_emp &gt; 0, healthcare_emp &gt; 0)\n# plot facet plot showing regression across years\nggplot(employment_data, aes(x = total_emp, y = healthcare_emp)) +\n  geom_point(alpha = 0.6, size = 2, color = \"#3b82f6\") +  #size and color\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#ef4444\", linewidth = 0.8) + #line width\n  facet_wrap(~YEAR, ncol = 3) +\n  labs( #labels \n    title = \"Healthcare Sector Employment Trends Across Metropolitan Areas\",\n    subtitle = \"Annual comparison by CBSA showing relationship with total workforce size\",\n    x = \"Total Employment\",\n    y = \"Healthcare Employment\",\n    caption = \"Data source: Bureau of Labor Statistics, Quarterly Census of Employment and Wages\"\n  ) +\n  scale_x_continuous(labels = comma_format()) +\n  scale_y_continuous(labels = comma_format()) +\n  theme_minimal(base_size = 12) +  # font size\n  theme( #title, axis, subtitle, text label font size\n    plot.title = element_text(face = \"bold\", size = 16), \n    plot.subtitle = element_text(size = 12, color = \"gray40\"), \n    axis.title = element_text(face = \"bold\", size = 11),  \n    axis.text = element_text(size = 9),  \n    strip.text = element_text(face = \"bold\", size = 11),  \n    panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45, hjust = 1)\n)\n\n\n\n\n\n\n\n\n\n\n\n3. The evolution of average household size over time. Use different lines to represent different CBSAs.\n\n\nShow code\nsuppressMessages(ensure_package(gghighlight))\n\n# Join population and household data\nhousehold_size &lt;- POPULATION |&gt;\n  inner_join(HOUSEHOLDS, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  mutate(avg_household_size = population / households) |&gt;\n  filter(!is.na(avg_household_size), avg_household_size &gt; 0)\n\n# Create highlighted version with custom groupings\nhousehold_size_highlighted &lt;- household_size |&gt;\n  mutate(\n    highlight_group = case_when(\n      str_detect(NAME, \"NY-NJ-PA Metro Area\") ~ \"New York City\",\n      str_detect(NAME, \"Los Angeles\") ~ \"Los Angeles\",\n      TRUE ~ \"Other CBSAs\"\n    )\n  )\n\n# Line plot of average household size trends\nggplot(household_size_highlighted, \n       aes(x = year, y = avg_household_size, group = NAME, color = highlight_group)) +\n  geom_line(aes(alpha = highlight_group, linewidth = highlight_group)) +\n  scale_color_manual(\n    values = c(\"New York City\" = \"#ef4444\", \n               \"Los Angeles\" = \"#3b82f6\", \n               \"Other CBSAs\" = \"gray70\"),\n    name = \"\"\n  ) +\n  scale_alpha_manual(\n    values = c(\"New York City\" = 1, \n               \"Los Angeles\" = 1, \n               \"Other CBSAs\" = 0.3),\n    guide = \"none\"\n  ) +\n  scale_linewidth_manual(\n    values = c(\"New York City\" = 1.2, \n               \"Los Angeles\" = 1.2, \n               \"Other CBSAs\" = 0.5),\n    guide = \"none\"\n  ) +\n  scale_x_continuous(breaks = seq(2009, 2023, 2)) +\n  labs(\n    title = \"Household Size Trends in Major Metropolitan Areas\",\n    subtitle = \"Comparing New York City and Los Angeles against all U.S. CBSAs, 2009-2023\",\n    x = \"Year\",\n    y = \"Average Household Size\",\n    caption = \"Data source: American Community Survey 1-year estimates\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, margin = margin(b = 5)),\n    plot.subtitle = element_text(size = 12, color = \"gray40\", margin = margin(b = 15)),\n    plot.caption = element_text(size = 9, color = \"gray50\", hjust = 0, margin = margin(t = 10)),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"bottom\",\n    legend.text = element_text(size = 11),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"gray90\")\n  )"
  },
  {
    "objectID": "mp02.html#building-indices-of-housing-affordability-and-housing-stock-growth",
    "href": "mp02.html#building-indices-of-housing-affordability-and-housing-stock-growth",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Building Indices of Housing Affordability and Housing Stock Growth",
    "text": "Building Indices of Housing Affordability and Housing Stock Growth\n\nRent Burden Analysis\n\n\nShow code\n# Join income and rent data, calculate rent-to-income ratio\nrent_burden &lt;- RENT |&gt;\n  inner_join(INCOME, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  filter(!is.na(monthly_rent), !is.na(household_income)) |&gt;\n  filter(monthly_rent &gt; 0, household_income &gt; 0) |&gt;\n  transmute(\n    GEOID, \n    NAME = enc2utf8(NAME), \n    year,\n    monthly_rent,\n    household_income,\n    rent_to_income = (monthly_rent * 12) / household_income\n  )\n\n# Standardize to 0-100 index using min-max scaling\nrb_range &lt;- range(rent_burden$rent_to_income, na.rm = TRUE)\n\nrent_burden &lt;- rent_burden |&gt;\n  mutate(\n    rent_burden_index = rescale(rent_to_income, to = c(0, 100), from = rb_range)\n  )\n\n# Create ranking table for latest year\nlatest_year &lt;- max(rent_burden$year, na.rm = TRUE)\n\nrent_rank &lt;- rent_burden |&gt;\n  filter(year == latest_year) |&gt;\n  arrange(desc(rent_burden_index)) |&gt;\n  mutate(\n    NAME = enc2utf8(NAME),\n    latest_rent_burden = round(rent_burden_index, 2),\n    rent_to_income = round(rent_to_income, 4)\n  ) |&gt;\n  select(NAME, latest_rent_burden, rent_to_income)\n\n# Combine top and bottom CBSAs\nrent_rank_tbl &lt;- bind_rows(\n  head(rent_rank, 10) |&gt; mutate(Category = \"Highest Burden\"),\n  tail(rent_rank, 10) |&gt; mutate(Category = \"Lowest Burden\")\n)\n\n# Display table\ndatatable(\n  rent_rank_tbl,\n  options = list(\n    pageLength = 10,\n    autoWidth = TRUE,\n    columnDefs = list(\n      list(className = \"dt-right\", targets = c(1, 2))\n    )\n  ),\n  caption = \"Metropolitan Areas with Highest and Lowest Rent Burden (2023)\",\n  colnames = c(\"Metro Area\", \"Rent Burden Index\", \"Rent-to-Income Ratio\", \"Category\"),\n  rownames = FALSE\n)\n\n\n\n\n\n\nThe rent burden index shows how much of the average household’s income goes toward rent, scored from 0 to 100. A higher score means people are spending a larger share of their paycheck on housing. This makes it a helpful way to compare how heavy the rent burden is across different cities, even when those cities have very different wages or housing markets."
  },
  {
    "objectID": "mp02.html#housing-burden",
    "href": "mp02.html#housing-burden",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Housing Burden",
    "text": "Housing Burden\n\n\nShow code\nhousing_growth &lt;- POPULATION |&gt;\n  inner_join(PERMITS, by = c(\"GEOID\" = \"CBSA\", \"year\")) |&gt;\n  arrange(GEOID, year) |&gt;\n  group_by(GEOID, NAME) |&gt;\n  mutate(\n    # Calculate 5-year lagged population\n    population_5yr_ago = lag(population, n = 5),\n    # Calculate 5-year population growth\n    population_growth_5yr = population - population_5yr_ago,\n    # Calculate percentage growth over 5 years\n    population_growth_pct_5yr = (population_growth_5yr / population_5yr_ago) * 100\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year &gt;= 2014)  # Start from 2014 when 5-year lookback is available\n\n# Calculate permits per 1,000 residents (raw metric)\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    permits_per_1000 = (new_housing_units_permitted / population) * 1000\n  )\n\n# Standardize to 0-100 scale\ninstant_range &lt;- range(housing_growth$permits_per_1000, na.rm = TRUE)\n\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    instant_growth_index = rescale(permits_per_1000, \n                                   to = c(0, 100), \n                                   from = instant_range)\n  )\n\n### Step 3: Rate-Based Housing Growth Metric\n\n# Calculate permits relative to population growth\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    # Ratio of permits to population growth\n    permits_to_growth_ratio = case_when(\n      population_growth_5yr &gt; 100 ~ new_housing_units_permitted / population_growth_5yr,\n      TRUE ~ permits_per_1000 / 10  \n    )\n  )\n\n# Standardize to 0-100 scale\nrate_range &lt;- range(housing_growth$permits_to_growth_ratio, na.rm = TRUE)\n\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    rate_growth_index = rescale(permits_to_growth_ratio, \n                                to = c(0, 100), \n                                from = rate_range)\n  )\n\n# Weighted average: 40% instantaneous, 60% rate-based\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    composite_growth_score = (0.4 * instant_growth_index) + \n                            (0.6 * rate_growth_index)\n  )\n\nrecent_years &lt;- housing_growth |&gt;\n  filter(year &gt;= 2019, year &lt;= 2023) |&gt;\n  group_by(GEOID, NAME) |&gt;\n  summarize(\n    avg_instant_index = mean(instant_growth_index, na.rm = TRUE),\n    avg_rate_index = mean(rate_growth_index, na.rm = TRUE),\n    avg_composite_score = mean(composite_growth_score, na.rm = TRUE),\n    avg_permits_per_1000 = mean(permits_per_1000, na.rm = TRUE),\n    avg_population = mean(population, na.rm = TRUE),\n    avg_new_permits = mean(new_housing_units_permitted, na.rm = TRUE),\n    .groups = 'drop'\n  ) |&gt;\n  mutate(NAME = enc2utf8(NAME))\n\n\n\nInstantaneous: Highest and Lowest CBSAs\n\n\nShow code\ninstant_combined &lt;- bind_rows(\n  recent_years |&gt;\n    arrange(desc(avg_instant_index)) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Highest Growth\"),\n  recent_years |&gt;\n    arrange(avg_instant_index) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Lowest Growth\")\n) |&gt;\n  mutate(\n    avg_instant_index = round(avg_instant_index, 1),\n    avg_permits_per_1000 = round(avg_permits_per_1000, 2),\n    avg_population = comma(round(avg_population, 0))\n  ) |&gt;\n  select(NAME, avg_instant_index, avg_permits_per_1000, avg_population, Category)\n\ndatatable(\n  instant_combined,\n  caption = \"Instantaneous Housing Growth Index: Top and Bottom 10 CBSAs (2019-2023 Average)\",\n  colnames = c(\"Metro Area\", \"Growth Index\", \"Permits per 1,000\", \"Avg Population\", \"Category\"),\n  rownames = FALSE,\n  options = list(\n    pageLength = 20,\n    columnDefs = list(\n      list(className = \"dt-right\", targets = c(1, 2, 3))\n    )\n  ),\n  filter = 'top'\n)\n\n\n\n\n\n\n\n\nRate-Based: Highest and Lowest CBSAs\n\n\nShow code\nrate_combined &lt;- bind_rows(\n  recent_years |&gt;\n    arrange(desc(avg_rate_index)) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Highest Growth\"),\n  recent_years |&gt;\n    arrange(avg_rate_index) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Lowest Growth\")\n) |&gt;\n  mutate(\n    avg_rate_index = round(avg_rate_index, 1),\n    avg_population = comma(round(avg_population, 0))\n  ) |&gt;\n  select(NAME, avg_rate_index, avg_population, Category)\n\ndatatable(\n  rate_combined,\n  caption = \"Rate-Based Housing Growth Index: Top and Bottom 10 CBSAs (2019-2023 Average)\",\n  colnames = c(\"Metro Area\", \"Growth Index\", \"Avg Population\", \"Category\"),\n  rownames = FALSE,\n  options = list(\n    pageLength = 20,\n    columnDefs = list(\n      list(className = \"dt-right\", targets = c(1, 2))\n    )\n  ),\n  filter = 'top'\n)\n\n\n\n\n\n\n\n\nComposite Growth Score: Highest and Lowest CBSAs\n\n\nShow code\ncomposite_combined &lt;- bind_rows(\n  recent_years |&gt;\n    arrange(desc(avg_composite_score)) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Highest Growth\"),\n  recent_years |&gt;\n    arrange(avg_composite_score) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Lowest Growth\")\n) |&gt;\n  mutate(\n    avg_composite_score = round(avg_composite_score, 1),\n    avg_instant_index = round(avg_instant_index, 1),\n    avg_rate_index = round(avg_rate_index, 1),\n    avg_population = comma(round(avg_population, 0))\n  ) |&gt;\n  select(NAME, avg_composite_score, avg_instant_index, avg_rate_index, avg_population, Category)\n\ndatatable(\n  composite_combined,\n  caption = \"Composite Housing Growth Score: Top and Bottom 10 CBSAs (2019-2023 Average)\",\n  colnames = c(\"Metro Area\", \"Composite Score\", \"Instant Index\", \"Rate Index\", \"Avg Population\", \"Category\"),\n  rownames = FALSE,\n  options = list(\n    pageLength = 20,\n    columnDefs = list(\n      list(className = \"dt-right\", targets = c(1, 2, 3, 4))\n    )\n  ),\n  filter = 'top'\n)"
  },
  {
    "objectID": "mp02.html#visualization",
    "href": "mp02.html#visualization",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Visualization",
    "text": "Visualization\n\nHousing Growth vs Change in Rent Burden\n\n\nShow code\nrent_burden &lt;- INCOME |&gt;\n  select(GEOID, NAME, year, household_income) |&gt;\n  inner_join(RENT |&gt; select(GEOID, year, monthly_rent), join_by(GEOID, year)) |&gt;\n  mutate(rent_burden_share = monthly_rent / (household_income / 12)) |&gt;\n  filter(is.finite(rent_burden_share))\n\nrb_change &lt;- rent_burden |&gt;\n  mutate(period = case_when(year %in% 2009:2011 ~ \"early\",\n                            year %in% 2021:2023 ~ \"late\",\n                            TRUE ~ NA_character_)) |&gt;\n  filter(!is.na(period)) |&gt;\n  group_by(GEOID, NAME, period) |&gt;\n  summarize(rent_burden_avg = mean(rent_burden_share, na.rm = TRUE), .groups = \"drop\") |&gt;\n  tidyr::pivot_wider(names_from = period, values_from = rent_burden_avg) |&gt;\n  mutate(rb_change = late - early) |&gt;\n  filter(is.finite(rb_change))\n\npermits_per_1k &lt;- PERMITS |&gt;\n  inner_join(POPULATION, join_by(CBSA == GEOID, year == year)) |&gt;\n  mutate(permits_per_1000 = 1000 * new_housing_units_permitted / population) |&gt;\n  filter(year &gt;= 2014, year &lt;= 2023) |&gt;\n  group_by(CBSA) |&gt;\n  summarize(avg_permits_per_1000 = mean(permits_per_1000, na.rm = TRUE), .groups = \"drop\")\n\nviz1_df &lt;- rb_change |&gt;\n  inner_join(permits_per_1k, join_by(GEOID == CBSA)) |&gt;\n  rename(cbsa = GEOID)\n\nx_ref &lt;- median(viz1_df$avg_permits_per_1000, na.rm = TRUE)\n\nggplot(viz1_df, aes(x = avg_permits_per_1000, y = rb_change)) +\n  geom_hline(yintercept = 0, linewidth = 0.4, linetype = \"dashed\") +\n  geom_vline(xintercept = x_ref, linewidth = 0.4, linetype = \"dashed\") +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Housing Growth vs Change in Rent Burden\",\n    subtitle = \"Right = more permits per 1,000 residents (2014–2023). Below zero = rent burden fell (2021–2023 vs 2009–2011).\",\n    x = \"Avg permits per 1,000 residents (2014–2023)\",\n    y = \"Change in rent burden share (late minus early)\"\n  ) +\n  scale_x_continuous(labels = label_number(accuracy = 0.1)) +\n  scale_y_continuous(labels = percent_format(accuracy = 0.1)) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nHousing Growth vs Population Growth\n\n\nShow code\npop_growth &lt;- POPULATION |&gt;\n  filter(year %in% c(2009, 2023)) |&gt;\n  select(GEOID, NAME, year, population) |&gt;\n  tidyr::pivot_wider(names_from = year, values_from = population, names_prefix = \"y_\") |&gt;\n  mutate(pop_growth_rate = (y_2023 - y_2009) / y_2009) |&gt;\n  filter(is.finite(pop_growth_rate))\n\nviz2_df &lt;- pop_growth |&gt;\n  inner_join(permits_per_1k, join_by(GEOID == CBSA))\n\nx_ref2 &lt;- median(viz2_df$pop_growth_rate, na.rm = TRUE)\ny_ref2 &lt;- median(viz2_df$avg_permits_per_1000, na.rm = TRUE)\n\nggplot(viz2_df, aes(x = pop_growth_rate, y = avg_permits_per_1000)) +\n  geom_vline(xintercept = x_ref2, linewidth = 0.4, linetype = \"dashed\") +\n  geom_hline(yintercept = y_ref2, linewidth = 0.4, linetype = \"dashed\") +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Population Growth vs Housing Permits\",\n    subtitle = \"Upper-right = growing population and high permitting\",\n    x = \"Population growth rate (2009–2023)\",\n    y = \"Avg permits per 1,000 residents (2014–2023)\"\n  ) +\n  scale_x_continuous(labels = percent_format(accuracy = 0.1)) +\n  scale_y_continuous(labels = label_number(accuracy = 0.1)) +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "mp02.html#policy-brief",
    "href": "mp02.html#policy-brief",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Policy Brief",
    "text": "Policy Brief\nHousing affordability has become one of the biggest economic problems in the country, and the patterns in the data are clear. Cities that allow new housing to be built have lower rent pressure and stronger long-term growth, while the places with the highest rents are the same ones that have restricted development for years. Over fifteen years of Census and BLS data, Houston stands out as a city that has grown quickly without letting rent spiral out of control, because it continues to approve enough housing to meet demand. New York, on the other hand, is an example of the opposite outcome, where slow construction and strict zoning have kept rents among the highest in the country.\nThe goal of the proposed federal YIMBY incentive bill is to reward cities that build enough housing to keep rents manageable, while pushing high-cost metros to change course. Instead of forcing cities to follow a single national rule, the plan ties federal grants and infrastructure funding to real progress in rent affordability and housing construction. The people who benefit most from this are not developers, but middle-class workers like nurses and teachers who are priced out of the cities they serve. Lower rent pressure works like an instant increase in take-home pay, without needing a larger public salary budget.\nTo track progress, the policy uses two simple metrics: a rent burden score that measures how much income the average household spends on rent, and a housing growth score that compares new homes built to both population size and population growth. These numbers make it easy to see which cities are actually becoming more affordable. A strong sponsorship team could pair a representative from Houston, which already shows how building more housing works, with someone from New York, where housing costs are still rising and reform is badly needed.\nExtra Credit #3: Millennial Appeal\nThe bill also includes a youth retention bonus for cities that grow their 25 to 34 population. Young adults are the group most affected by high rent, and they are also the group that drives future economic and cultural growth. Cities that build more housing are the ones that keep young workers, new families, and future taxpayers, which makes this policy not only about affordability but also about long-term economic health."
  }
]