[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Goal: Project Set-up for STA9750"
  },
  {
    "objectID": "projects.html#mini-project-00",
    "href": "projects.html#mini-project-00",
    "title": "",
    "section": "",
    "text": "Goal: Project Set-up for STA9750"
  },
  {
    "objectID": "projects.html#mini-project-01",
    "href": "projects.html#mini-project-01",
    "title": "",
    "section": "Mini-Project #1",
    "text": "Mini-Project #1\n\n\nGoal: Analyze Netflix’s Top 10 data to identify the most popular programming on the platform."
  },
  {
    "objectID": "projects.html#mini-project-02",
    "href": "projects.html#mini-project-02",
    "title": "",
    "section": "Mini-Project #2",
    "text": "Mini-Project #2\n\n\nGoal: Identify America’s most “YIMBY” cities using a variety of census data sources and real estate indices."
  },
  {
    "objectID": "mp02.html",
    "href": "mp02.html",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "",
    "text": "For this assignment, we will identify America’s most “YIMBY” cities using a variety of census data sources and real estate indices. Below, we will use the results of our analysis to lobby politicians in support of a federal YIMBY-incentive program. We will then prepare a short policy brief that is designed to help us find congressional representatives whose districts would benefit from adopting YIMBY-type policies and whose reelection prospects would be improved by sponsoring this type of bill."
  },
  {
    "objectID": "mp02.html#objective",
    "href": "mp02.html#objective",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "",
    "text": "For this assignment, we will identify America’s most “YIMBY” cities using a variety of census data sources and real estate indices. Below, we will use the results of our analysis to lobby politicians in support of a federal YIMBY-incentive program. We will then prepare a short policy brief that is designed to help us find congressional representatives whose districts would benefit from adopting YIMBY-type policies and whose reelection prospects would be improved by sponsoring this type of bill."
  },
  {
    "objectID": "mp02.html#data-acquistion",
    "href": "mp02.html#data-acquistion",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Data Acquistion",
    "text": "Data Acquistion\n\n\nShow code\nif(!dir.exists(file.path(\"data\", \"mp02\"))){\n    dir.create(file.path(\"data\", \"mp02\"), showWarnings=FALSE, recursive=TRUE)\n}\n\nensure_package &lt;- function(pkg){\n    pkg &lt;- as.character(substitute(pkg))\n    options(repos = c(CRAN = \"https://cloud.r-project.org\"))\n    if(!require(pkg, character.only=TRUE, quietly=TRUE)) install.packages(pkg)\n    stopifnot(require(pkg, character.only=TRUE, quietly=TRUE))\n}\n\nensure_package(tidyverse)\nensure_package(glue)\nensure_package(readxl)\nensure_package(tidycensus)\n\n\n\n\nShow code\nget_acs_all_years &lt;- function(variable, geography=\"cbsa\",\n                              start_year=2009, end_year=2023){\n    fname &lt;- glue(\"{variable}_{geography}_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        YEARS &lt;- seq(start_year, end_year)\n        YEARS &lt;- YEARS[YEARS != 2020] # Drop 2020 - No survey (covid)\n        \n        ALL_DATA &lt;- map(YEARS, function(yy){\n            tidycensus::get_acs(geography, variable, year=yy, survey=\"acs1\") |&gt;\n                mutate(year=yy) |&gt;\n                select(-moe, -variable) |&gt;\n                rename(!!variable := estimate)\n        }) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\n# Household income (12 month)\nINCOME &lt;- get_acs_all_years(\"B19013_001\") |&gt;\n    rename(household_income = B19013_001)\n\n# Monthly rent\nRENT &lt;- get_acs_all_years(\"B25064_001\") |&gt;\n    rename(monthly_rent = B25064_001)\n\n# Total population\nPOPULATION &lt;- get_acs_all_years(\"B01003_001\") |&gt;\n    rename(population = B01003_001)\n\n# Total number of households\nHOUSEHOLDS &lt;- get_acs_all_years(\"B11001_001\") |&gt;\n    rename(households = B11001_001)\n\n\n\n\nShow code\nget_building_permits &lt;- function(start_year = 2009, end_year = 2023){\n    fname &lt;- glue(\"housing_units_{start_year}_{end_year}.csv\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    if(!file.exists(fname)){\n        HISTORICAL_YEARS &lt;- seq(start_year, 2018)\n        \n        HISTORICAL_DATA &lt;- map(HISTORICAL_YEARS, function(yy){\n            historical_url &lt;- glue(\"https://www.census.gov/construction/bps/txt/tb3u{yy}.txt\")\n                \n            LINES &lt;- readLines(historical_url)[-c(1:11)]\n\n            CBSA_LINES &lt;- str_detect(LINES, \"^[[:digit:]]\")\n            CBSA &lt;- as.integer(str_sub(LINES[CBSA_LINES], 5, 10))\n\n            PERMIT_LINES &lt;- str_detect(str_sub(LINES, 48, 53), \"[[:digit:]]\")\n            PERMITS &lt;- as.integer(str_sub(LINES[PERMIT_LINES], 48, 53))\n            \n            data_frame(CBSA = CBSA,\n                       new_housing_units_permitted = PERMITS, \n                       year = yy)\n        }) |&gt; bind_rows()\n        \n        CURRENT_YEARS &lt;- seq(2019, end_year)\n        \n        CURRENT_DATA &lt;- map(CURRENT_YEARS, function(yy){\n            current_url &lt;- glue(\"https://www.census.gov/construction/bps/xls/msaannual_{yy}99.xls\")\n            \n            temp &lt;- tempfile()\n            \n            download.file(current_url, destfile = temp, mode=\"wb\")\n            \n            fallback &lt;- function(.f1, .f2){\n                function(...){\n                    tryCatch(.f1(...), \n                             error=function(e) .f2(...))\n                }\n            }\n            \n            reader &lt;- fallback(read_xlsx, read_xls)\n            \n            reader(temp, skip=5) |&gt;\n                na.omit() |&gt;\n                select(CBSA, Total) |&gt;\n                mutate(year = yy) |&gt;\n                rename(new_housing_units_permitted = Total)\n        }) |&gt; bind_rows()\n        \n        ALL_DATA &lt;- rbind(HISTORICAL_DATA, CURRENT_DATA)\n        \n        write_csv(ALL_DATA, fname)\n        \n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n}\n\nPERMITS &lt;- get_building_permits()\n\nensure_package(httr2)\nensure_package(rvest)\n\n\n\n\nShow code\nget_bls_industry_codes &lt;- function(){\n    fname &lt;- fname &lt;- file.path(\"data\", \"mp02\", \"bls_industry_codes.csv\")\n    \n    if(!file.exists(fname)){\n    \n        resp &lt;- request(\"https://www.bls.gov\") |&gt; \n            req_url_path(\"cew\", \"classifications\", \"industry\", \"industry-titles.htm\") |&gt;\n            req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n            req_error(is_error = \\(resp) FALSE) |&gt;\n            req_perform()\n        \n        resp_check_status(resp)\n        \n        naics_table &lt;- resp_body_html(resp) |&gt;\n            html_element(\"#naics_titles\") |&gt; \n            html_table() |&gt;\n            mutate(title = str_trim(str_remove(str_remove(`Industry Title`, Code), \"NAICS\"))) |&gt;\n            select(-`Industry Title`) |&gt;\n            mutate(depth = if_else(nchar(Code) &lt;= 5, nchar(Code) - 1, NA)) |&gt;\n            filter(!is.na(depth))\n        \n        naics_table &lt;- naics_table |&gt; \n            filter(depth == 4) |&gt; \n            rename(level4_title=title) |&gt; \n            mutate(level1_code = str_sub(Code, end=2), \n                   level2_code = str_sub(Code, end=3), \n                   level3_code = str_sub(Code, end=4)) |&gt;\n            left_join(naics_table, join_by(level1_code == Code)) |&gt;\n            rename(level1_title=title) |&gt;\n            left_join(naics_table, join_by(level2_code == Code)) |&gt;\n            rename(level2_title=title) |&gt;\n            left_join(naics_table, join_by(level3_code == Code)) |&gt;\n            rename(level3_title=title) |&gt;\n            select(-starts_with(\"depth\")) |&gt;\n            rename(level4_code = Code) |&gt;\n            select(level1_title, level2_title, level3_title, level4_title, \n                   level1_code,  level2_code,  level3_code,  level4_code)\n    \n        write_csv(naics_table, fname)\n    }\n    \n    read_csv(fname, show_col_types=FALSE)\n    \n}\n\nINDUSTRY_CODES &lt;- get_bls_industry_codes()\n\n\n\n\nShow code\nensure_package(httr2)\nensure_package(rvest)\nget_bls_qcew_annual_averages &lt;- function(start_year=2009, end_year=2023){\n    fname &lt;- glue(\"bls_qcew_{start_year}_{end_year}.csv.gz\")\n    fname &lt;- file.path(\"data\", \"mp02\", fname)\n    \n    YEARS &lt;- seq(start_year, end_year)\n    YEARS &lt;- YEARS[YEARS != 2020] # Drop Covid year to match ACS\n    \n    if(!file.exists(fname)){\n        ALL_DATA &lt;- map(YEARS, .progress=TRUE, possibly(function(yy){\n            fname_inner &lt;- file.path(\"data\", \"mp02\", glue(\"{yy}_qcew_annual_singlefile.zip\"))\n            \n            if(!file.exists(fname_inner)){\n                request(\"https://www.bls.gov\") |&gt; \n                    req_url_path(\"cew\", \"data\", \"files\", yy, \"csv\",\n                                 glue(\"{yy}_annual_singlefile.zip\")) |&gt;\n                    req_headers(`User-Agent` = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:143.0) Gecko/20100101 Firefox/143.0\") |&gt; \n                    req_retry(max_tries=5) |&gt;\n                    req_perform(fname_inner)\n            }\n            \n            if(file.info(fname_inner)$size &lt; 755e5){\n                warning(sQuote(fname_inner), \"appears corrupted. Please delete and retry this step.\")\n            }\n            \n            read_csv(fname_inner, \n                     show_col_types=FALSE) |&gt; \n                mutate(YEAR = yy) |&gt;\n                select(area_fips, \n                       industry_code, \n                       annual_avg_emplvl, \n                       total_annual_wages, \n                       YEAR) |&gt;\n                filter(nchar(industry_code) &lt;= 5, \n                       str_starts(area_fips, \"C\")) |&gt;\n                filter(str_detect(industry_code, \"-\", negate=TRUE)) |&gt;\n                mutate(FIPS = area_fips, \n                       INDUSTRY = as.integer(industry_code), \n                       EMPLOYMENT = as.integer(annual_avg_emplvl), \n                       TOTAL_WAGES = total_annual_wages) |&gt;\n                select(-area_fips, \n                       -industry_code, \n                       -annual_avg_emplvl, \n                       -total_annual_wages) |&gt;\n                # 10 is a special value: \"all industries\" , so omit\n                filter(INDUSTRY != 10) |&gt; \n                mutate(AVG_WAGE = TOTAL_WAGES / EMPLOYMENT)\n        })) |&gt; bind_rows()\n        \n        write_csv(ALL_DATA, fname)\n    }\n    \n    ALL_DATA &lt;- read_csv(fname, show_col_types=FALSE)\n    \n    ALL_DATA_YEARS &lt;- unique(ALL_DATA$YEAR)\n    \n    YEARS_DIFF &lt;- setdiff(YEARS, ALL_DATA_YEARS)\n    \n    if(length(YEARS_DIFF) &gt; 0){\n        stop(\"Download failed for the following years: \", YEARS_DIFF, \n             \". Please delete intermediate files and try again.\")\n    }\n    \n    ALL_DATA\n}\n\n\n\n\nShow code\nWAGES &lt;- get_bls_qcew_annual_averages()"
  },
  {
    "objectID": "mp02.html#exploratory-questions",
    "href": "mp02.html#exploratory-questions",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Exploratory Questions",
    "text": "Exploratory Questions\n\n1. Which CBSA (by name) permitted the largest number of new housing units in the decade from 2010 to 2019 (inclusive)?\n\n\nShow code\nlibrary(dplyr)\nlibrary(DT)\nlibrary(scales)\n\nPERMITS |&gt;\n  filter(year &gt;= 2010, year &lt;= 2019) |&gt;\n  group_by(CBSA) |&gt;\n  summarize(`New Housing Units Permitted` = sum(new_housing_units_permitted)) |&gt;\n  left_join(INCOME |&gt; distinct(GEOID, NAME), join_by(CBSA == GEOID)) |&gt;\n  arrange(desc(`New Housing Units Permitted`)) |&gt;\n  rename(\n    Name = NAME\n  ) |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\nCBSA 26420 permitted the largest number of new housing units in the decade from 2010 to 2019.\n\n\n2. In what year did Albuquerque, NM (CBSA Number 10740) permit the most new housing units?\n\n\nShow code\n# Creates summary table\nabq_summary &lt;- PERMITS |&gt;\n  filter(CBSA == 10740, year &gt;= 2010, year &lt;= 2019) |&gt;\n  group_by(year) |&gt;\n  summarise(total_units = sum(new_housing_units_permitted), .groups = \"drop\")\n\n# Preview table (pretty column names only for display)\nabq_summary |&gt;\n  arrange(desc(total_units)) |&gt;\n  rename(Year = year, `Total Units` = total_units) |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\nShow code\n# Values for inline text\nabq_top   &lt;- slice_max(abq_summary, order_by = total_units, n = 1, with_ties = FALSE)\nabq_year  &lt;- abq_top$year\nabq_units &lt;- abq_top$total_units\n\n\nAlbuquerque, NM permitted the most new housing units in 2013, with 2606 units approved.\n\n\n3. Which state (not CBSA) had the highest average individual income in 2015?\n\n\nShow code\nstate_income_2015 &lt;- INCOME |&gt;\nfilter(year == 2015) |&gt;\nleft_join(HOUSEHOLDS |&gt; filter(year == 2015), join_by(GEOID)) |&gt;\nleft_join(POPULATION  |&gt; filter(year == 2015), join_by(GEOID)) |&gt;\nmutate(\nName = str_extract(NAME.x, \", (.{2})\", group = 1),\n`Total Income (CBSA)` = household_income * households,\n`Average Individual Income` = `Total Income (CBSA)` / population\n) |&gt;\nselect(Name, `Average Individual Income`) |&gt;\nfilter(!is.na(`Average Individual Income`)) |&gt;\narrange(desc(`Average Individual Income`)) |&gt;\nmutate(`Average Individual Income` = dollar(round(`Average Individual Income`, 2)))\n\ndatatable(\nhead(state_income_2015, 10),\noptions = list(pageLength = 10, searching = FALSE, info = FALSE),\ncaption = \"Top States by Average Individual Income (2015)\"\n)\n\n\n\n\n\n\nIn 2015, CA (California) had the highest per-capita income.\n\n\n4. What is the last year in which the NYC CBSA had the most data scientists in the country?\n\n\nShow code\nt1 &lt;- INCOME |&gt; \n  mutate(std_cbsa = paste0(\"C\", GEOID))\n\nt2 &lt;- WAGES |&gt; \n  mutate(std_cbsa = paste0(FIPS, \"0\"))\n\ninner_join(t1, t2, join_by(std_cbsa == std_cbsa)) |&gt;\n  filter(INDUSTRY == 5182) |&gt;\n  group_by(YEAR, std_cbsa) |&gt;\n  summarize(`Employment Number` = sum(EMPLOYMENT)) |&gt;\n  arrange(YEAR, desc(`Employment Number`)) |&gt;\n  filter(`Employment Number` == first(`Employment Number`)) |&gt;\n  filter(std_cbsa == \"C35620\") |&gt;\n  arrange(desc(`Employment Number`)) |&gt;\n  rename(\n    Year = YEAR,\n    CBSA = std_cbsa\n  ) |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\n2015 is the last year in which the NYC CBSA had the most data scientists in the country.\n\n\n5. What fraction of total wages in the NYC CBSA was earned by people employed in the finance and insurance industries (NAICS code 52)? In what year did this fraction peak?\n\n\nShow code\nWAGES |&gt;\n  filter(FIPS == \"C3562\") |&gt;\n  group_by(YEAR) |&gt;\n  summarize(\n    `Total Wages` = sum(TOTAL_WAGES),\n    `Finance and Insurance Wages` = sum(ifelse(INDUSTRY == 52, TOTAL_WAGES, 0)),\n    `Share of Finance & Insurance` = `Finance and Insurance Wages` / `Total Wages`,\n    .groups = \"drop\"\n  ) |&gt;\n  mutate(\n    `Total Wages` = scales::dollar(`Total Wages`),\n    `Finance and Insurance Wages` = scales::dollar(`Finance and Insurance Wages`),\n    `Share of Finance & Insurance` = scales::percent(`Share of Finance & Insurance`, accuracy = 0.1)\n  ) |&gt;\n  arrange(desc(`Share of Finance & Insurance`)) |&gt;\n  datatable(options = list(searching = FALSE, info = FALSE))\n\n\n\n\n\n\nThe fractions of total wages in the NYC CBSA that was earned by people employed in the finance/insurance industries peaked in 2014."
  },
  {
    "objectID": "mp02.html#visualizations",
    "href": "mp02.html#visualizations",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Visualizations",
    "text": "Visualizations\n\n1. The relationship between monthly rent and average household income per CBSA in 2009.\n\n\nShow code\nlibrary(ggplot2)\n\nQ1 &lt;- INCOME |&gt; \n  filter(year == 2009) |&gt;\n  inner_join(RENT |&gt; filter(year == 2009), join_by(GEOID == GEOID)) |&gt;\n  select(NAME.x, household_income, monthly_rent)\n\nggplot(Q1, aes(x = household_income, y = monthly_rent)) +\n  geom_point(alpha = 0.35, size = 2.8, color = \"steelblue4\") +\n  \n  stat_smooth(se = FALSE, color = \"red4\", linewidth = 1.2) +\n  \n  scale_y_continuous(labels = scales::dollar) +\n  scale_x_continuous(labels = scales::dollar) +\n  \n  labs(\n    title = \"Relationship Between Monthly Rent and Average Household Income (2009)\",\n    subtitle = \"Each point represents a U.S. CBSA (Core-Based Statistical Area)\",\n    x = \"Average Household Income (USD)\",\n    y = \"Average Monthly Rent (USD)\"\n  ) +\n  \n  theme_bw(base_size = 13) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 15),\n    plot.subtitle = element_text(size = 12, color = \"gray30\"),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"gray90\"),\n    axis.title = element_text(face = \"bold\"),\n    plot.margin = margin(10, 15, 10, 15)\n  )\n\n\n\n\n\n\n\n\n\n\n\n2. The relationship between total employment and total employment in the health care and social services sector (NAICS 62) across different CBSAs.\n\n\nShow code\n# Get total employment by CBSA and year \n#| fig-width: 15 #set dimensions of figure\n#| fig-height: 15\n#sum wages by fips and year\ntotal_employment &lt;- WAGES |&gt;\n  group_by(FIPS, YEAR) |&gt; \n  summarize(total_emp = sum(EMPLOYMENT, na.rm = TRUE), .groups = 'drop')\n# Get healthcare employment, NAICS 62. sum by fips and year\nhealthcare_employment &lt;- WAGES |&gt;\n  filter(str_starts(as.character(INDUSTRY), \"62\")) |&gt;\n  group_by(FIPS, YEAR) |&gt;\n  summarize(healthcare_emp = sum(EMPLOYMENT, na.rm = TRUE), .groups = 'drop')\n# inner join and filter where employment is greater than 0 \nemployment_data &lt;- total_employment |&gt;\n  inner_join(healthcare_employment, by = c(\"FIPS\", \"YEAR\")) |&gt;\n  filter(total_emp &gt; 0, healthcare_emp &gt; 0)\n# plot facet plot showing regression across years\nggplot(employment_data, aes(x = total_emp, y = healthcare_emp)) +\n  geom_point(alpha = 0.6, size = 2, color = \"#3b82f6\") +  #size and color\n  geom_smooth(method = \"lm\", se = TRUE, color = \"#ef4444\", linewidth = 0.8) + #line width\n  facet_wrap(~YEAR, ncol = 3) +\n  labs( #labels \n    title = \"Healthcare Sector Employment Trends Across Metropolitan Areas\",\n    subtitle = \"Annual comparison by CBSA showing relationship with total workforce size\",\n    x = \"Total Employment\",\n    y = \"Healthcare Employment\",\n    caption = \"Data source: Bureau of Labor Statistics, Quarterly Census of Employment and Wages\"\n  ) +\n  scale_x_continuous(labels = comma_format()) +\n  scale_y_continuous(labels = comma_format()) +\n  theme_minimal(base_size = 12) +  # font size\n  theme( #title, axis, subtitle, text label font size\n    plot.title = element_text(face = \"bold\", size = 16), \n    plot.subtitle = element_text(size = 12, color = \"gray40\"), \n    axis.title = element_text(face = \"bold\", size = 11),  \n    axis.text = element_text(size = 9),  \n    strip.text = element_text(face = \"bold\", size = 11),  \n    panel.grid.minor = element_blank(), axis.text.x = element_text(angle = 45, hjust = 1)\n)\n\n\n\n\n\n\n\n\n\n\n\n3. The evolution of average household size over time. Use different lines to represent different CBSAs.\n\n\nShow code\nsuppressMessages(ensure_package(gghighlight))\n\n# Join population and household data\nhousehold_size &lt;- POPULATION |&gt;\n  inner_join(HOUSEHOLDS, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  mutate(avg_household_size = population / households) |&gt;\n  filter(!is.na(avg_household_size), avg_household_size &gt; 0)\n\n# Create highlighted version with custom groupings\nhousehold_size_highlighted &lt;- household_size |&gt;\n  mutate(\n    highlight_group = case_when(\n      str_detect(NAME, \"NY-NJ-PA Metro Area\") ~ \"New York City\",\n      str_detect(NAME, \"Los Angeles\") ~ \"Los Angeles\",\n      TRUE ~ \"Other CBSAs\"\n    )\n  )\n\n# Line plot of average household size trends\nggplot(household_size_highlighted, \n       aes(x = year, y = avg_household_size, group = NAME, color = highlight_group)) +\n  geom_line(aes(alpha = highlight_group, linewidth = highlight_group)) +\n  scale_color_manual(\n    values = c(\"New York City\" = \"#ef4444\", \n               \"Los Angeles\" = \"#3b82f6\", \n               \"Other CBSAs\" = \"gray70\"),\n    name = \"\"\n  ) +\n  scale_alpha_manual(\n    values = c(\"New York City\" = 1, \n               \"Los Angeles\" = 1, \n               \"Other CBSAs\" = 0.3),\n    guide = \"none\"\n  ) +\n  scale_linewidth_manual(\n    values = c(\"New York City\" = 1.2, \n               \"Los Angeles\" = 1.2, \n               \"Other CBSAs\" = 0.5),\n    guide = \"none\"\n  ) +\n  scale_x_continuous(breaks = seq(2009, 2023, 2)) +\n  labs(\n    title = \"Household Size Trends in Major Metropolitan Areas\",\n    subtitle = \"Comparing New York City and Los Angeles against all U.S. CBSAs, 2009-2023\",\n    x = \"Year\",\n    y = \"Average Household Size\",\n    caption = \"Data source: American Community Survey 1-year estimates\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16, margin = margin(b = 5)),\n    plot.subtitle = element_text(size = 12, color = \"gray40\", margin = margin(b = 15)),\n    plot.caption = element_text(size = 9, color = \"gray50\", hjust = 0, margin = margin(t = 10)),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    legend.position = \"bottom\",\n    legend.text = element_text(size = 11),\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(color = \"gray90\")\n  )"
  },
  {
    "objectID": "mp02.html#building-indices-of-housing-affordability-and-housing-stock-growth",
    "href": "mp02.html#building-indices-of-housing-affordability-and-housing-stock-growth",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Building Indices of Housing Affordability and Housing Stock Growth",
    "text": "Building Indices of Housing Affordability and Housing Stock Growth\n\nRent Burden Analysis\n\n\nShow code\n# Join income and rent data, calculate rent-to-income ratio\nrent_burden &lt;- RENT |&gt;\n  inner_join(INCOME, by = c(\"GEOID\", \"NAME\", \"year\")) |&gt;\n  filter(!is.na(monthly_rent), !is.na(household_income)) |&gt;\n  filter(monthly_rent &gt; 0, household_income &gt; 0) |&gt;\n  transmute(\n    GEOID, \n    NAME = enc2utf8(NAME), \n    year,\n    monthly_rent,\n    household_income,\n    rent_to_income = (monthly_rent * 12) / household_income\n  )\n\n# Standardize to 0-100 index using min-max scaling\nrb_range &lt;- range(rent_burden$rent_to_income, na.rm = TRUE)\n\nrent_burden &lt;- rent_burden |&gt;\n  mutate(\n    rent_burden_index = rescale(rent_to_income, to = c(0, 100), from = rb_range)\n  )\n\n# Create ranking table for latest year\nlatest_year &lt;- max(rent_burden$year, na.rm = TRUE)\n\nrent_rank &lt;- rent_burden |&gt;\n  filter(year == latest_year) |&gt;\n  arrange(desc(rent_burden_index)) |&gt;\n  mutate(\n    NAME = enc2utf8(NAME),\n    latest_rent_burden = round(rent_burden_index, 2),\n    rent_to_income = round(rent_to_income, 4)\n  ) |&gt;\n  select(NAME, latest_rent_burden, rent_to_income)\n\n# Combine top and bottom CBSAs\nrent_rank_tbl &lt;- bind_rows(\n  head(rent_rank, 10) |&gt; mutate(Category = \"Highest Burden\"),\n  tail(rent_rank, 10) |&gt; mutate(Category = \"Lowest Burden\")\n)\n\n# Display table\ndatatable(\n  rent_rank_tbl,\n  options = list(\n    pageLength = 10,\n    autoWidth = TRUE,\n    columnDefs = list(\n      list(className = \"dt-right\", targets = c(1, 2))\n    )\n  ),\n  caption = \"Metropolitan Areas with Highest and Lowest Rent Burden (2023)\",\n  colnames = c(\"Metro Area\", \"Rent Burden Index\", \"Rent-to-Income Ratio\", \"Category\"),\n  rownames = FALSE\n)\n\n\n\n\n\n\nThe rent burden index shows how much of the average household’s income goes toward rent, scored from 0 to 100. A higher score means people are spending a larger share of their paycheck on housing. This makes it a helpful way to compare how heavy the rent burden is across different cities, even when those cities have very different wages or housing markets."
  },
  {
    "objectID": "mp02.html#housing-burden",
    "href": "mp02.html#housing-burden",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Housing Burden",
    "text": "Housing Burden\n\n\nShow code\nhousing_growth &lt;- POPULATION |&gt;\n  inner_join(PERMITS, by = c(\"GEOID\" = \"CBSA\", \"year\")) |&gt;\n  arrange(GEOID, year) |&gt;\n  group_by(GEOID, NAME) |&gt;\n  mutate(\n    # Calculate 5-year lagged population\n    population_5yr_ago = lag(population, n = 5),\n    # Calculate 5-year population growth\n    population_growth_5yr = population - population_5yr_ago,\n    # Calculate percentage growth over 5 years\n    population_growth_pct_5yr = (population_growth_5yr / population_5yr_ago) * 100\n  ) |&gt;\n  ungroup() |&gt;\n  filter(year &gt;= 2014)  # Start from 2014 when 5-year lookback is available\n\n# Calculate permits per 1,000 residents (raw metric)\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    permits_per_1000 = (new_housing_units_permitted / population) * 1000\n  )\n\n# Standardize to 0-100 scale\ninstant_range &lt;- range(housing_growth$permits_per_1000, na.rm = TRUE)\n\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    instant_growth_index = rescale(permits_per_1000, \n                                   to = c(0, 100), \n                                   from = instant_range)\n  )\n\n### Step 3: Rate-Based Housing Growth Metric\n\n# Calculate permits relative to population growth\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    # Ratio of permits to population growth\n    permits_to_growth_ratio = case_when(\n      population_growth_5yr &gt; 100 ~ new_housing_units_permitted / population_growth_5yr,\n      TRUE ~ permits_per_1000 / 10  \n    )\n  )\n\n# Standardize to 0-100 scale\nrate_range &lt;- range(housing_growth$permits_to_growth_ratio, na.rm = TRUE)\n\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    rate_growth_index = rescale(permits_to_growth_ratio, \n                                to = c(0, 100), \n                                from = rate_range)\n  )\n\n# Weighted average: 40% instantaneous, 60% rate-based\nhousing_growth &lt;- housing_growth |&gt;\n  mutate(\n    composite_growth_score = (0.4 * instant_growth_index) + \n                            (0.6 * rate_growth_index)\n  )\n\nrecent_years &lt;- housing_growth |&gt;\n  filter(year &gt;= 2019, year &lt;= 2023) |&gt;\n  group_by(GEOID, NAME) |&gt;\n  summarize(\n    avg_instant_index = mean(instant_growth_index, na.rm = TRUE),\n    avg_rate_index = mean(rate_growth_index, na.rm = TRUE),\n    avg_composite_score = mean(composite_growth_score, na.rm = TRUE),\n    avg_permits_per_1000 = mean(permits_per_1000, na.rm = TRUE),\n    avg_population = mean(population, na.rm = TRUE),\n    avg_new_permits = mean(new_housing_units_permitted, na.rm = TRUE),\n    .groups = 'drop'\n  ) |&gt;\n  mutate(NAME = enc2utf8(NAME))\n\n\n\nInstantaneous: Highest and Lowest CBSAs\n\n\nShow code\ninstant_combined &lt;- bind_rows(\n  recent_years |&gt;\n    arrange(desc(avg_instant_index)) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Highest Growth\"),\n  recent_years |&gt;\n    arrange(avg_instant_index) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Lowest Growth\")\n) |&gt;\n  mutate(\n    avg_instant_index = round(avg_instant_index, 1),\n    avg_permits_per_1000 = round(avg_permits_per_1000, 2),\n    avg_population = comma(round(avg_population, 0))\n  ) |&gt;\n  select(NAME, avg_instant_index, avg_permits_per_1000, avg_population, Category)\n\ndatatable(\n  instant_combined,\n  caption = \"Instantaneous Housing Growth Index: Top and Bottom 10 CBSAs (2019-2023 Average)\",\n  colnames = c(\"Metro Area\", \"Growth Index\", \"Permits per 1,000\", \"Avg Population\", \"Category\"),\n  rownames = FALSE,\n  options = list(\n    pageLength = 20,\n    columnDefs = list(\n      list(className = \"dt-right\", targets = c(1, 2, 3))\n    )\n  ),\n  filter = 'top'\n)\n\n\n\n\n\n\n\n\nRate-Based: Highest and Lowest CBSAs\n\n\nShow code\nrate_combined &lt;- bind_rows(\n  recent_years |&gt;\n    arrange(desc(avg_rate_index)) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Highest Growth\"),\n  recent_years |&gt;\n    arrange(avg_rate_index) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Lowest Growth\")\n) |&gt;\n  mutate(\n    avg_rate_index = round(avg_rate_index, 1),\n    avg_population = comma(round(avg_population, 0))\n  ) |&gt;\n  select(NAME, avg_rate_index, avg_population, Category)\n\ndatatable(\n  rate_combined,\n  caption = \"Rate-Based Housing Growth Index: Top and Bottom 10 CBSAs (2019-2023 Average)\",\n  colnames = c(\"Metro Area\", \"Growth Index\", \"Avg Population\", \"Category\"),\n  rownames = FALSE,\n  options = list(\n    pageLength = 20,\n    columnDefs = list(\n      list(className = \"dt-right\", targets = c(1, 2))\n    )\n  ),\n  filter = 'top'\n)\n\n\n\n\n\n\n\n\nComposite Growth Score: Highest and Lowest CBSAs\n\n\nShow code\ncomposite_combined &lt;- bind_rows(\n  recent_years |&gt;\n    arrange(desc(avg_composite_score)) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Highest Growth\"),\n  recent_years |&gt;\n    arrange(avg_composite_score) |&gt;\n    head(10) |&gt;\n    mutate(Category = \"Lowest Growth\")\n) |&gt;\n  mutate(\n    avg_composite_score = round(avg_composite_score, 1),\n    avg_instant_index = round(avg_instant_index, 1),\n    avg_rate_index = round(avg_rate_index, 1),\n    avg_population = comma(round(avg_population, 0))\n  ) |&gt;\n  select(NAME, avg_composite_score, avg_instant_index, avg_rate_index, avg_population, Category)\n\ndatatable(\n  composite_combined,\n  caption = \"Composite Housing Growth Score: Top and Bottom 10 CBSAs (2019-2023 Average)\",\n  colnames = c(\"Metro Area\", \"Composite Score\", \"Instant Index\", \"Rate Index\", \"Avg Population\", \"Category\"),\n  rownames = FALSE,\n  options = list(\n    pageLength = 20,\n    columnDefs = list(\n      list(className = \"dt-right\", targets = c(1, 2, 3, 4))\n    )\n  ),\n  filter = 'top'\n)"
  },
  {
    "objectID": "mp02.html#visualization",
    "href": "mp02.html#visualization",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Visualization",
    "text": "Visualization\n\nHousing Growth vs Change in Rent Burden\n\n\nShow code\nrent_burden &lt;- INCOME |&gt;\n  select(GEOID, NAME, year, household_income) |&gt;\n  inner_join(RENT |&gt; select(GEOID, year, monthly_rent), join_by(GEOID, year)) |&gt;\n  mutate(rent_burden_share = monthly_rent / (household_income / 12)) |&gt;\n  filter(is.finite(rent_burden_share))\n\nrb_change &lt;- rent_burden |&gt;\n  mutate(period = case_when(year %in% 2009:2011 ~ \"early\",\n                            year %in% 2021:2023 ~ \"late\",\n                            TRUE ~ NA_character_)) |&gt;\n  filter(!is.na(period)) |&gt;\n  group_by(GEOID, NAME, period) |&gt;\n  summarize(rent_burden_avg = mean(rent_burden_share, na.rm = TRUE), .groups = \"drop\") |&gt;\n  tidyr::pivot_wider(names_from = period, values_from = rent_burden_avg) |&gt;\n  mutate(rb_change = late - early) |&gt;\n  filter(is.finite(rb_change))\n\npermits_per_1k &lt;- PERMITS |&gt;\n  inner_join(POPULATION, join_by(CBSA == GEOID, year == year)) |&gt;\n  mutate(permits_per_1000 = 1000 * new_housing_units_permitted / population) |&gt;\n  filter(year &gt;= 2014, year &lt;= 2023) |&gt;\n  group_by(CBSA) |&gt;\n  summarize(avg_permits_per_1000 = mean(permits_per_1000, na.rm = TRUE), .groups = \"drop\")\n\nviz1_df &lt;- rb_change |&gt;\n  inner_join(permits_per_1k, join_by(GEOID == CBSA)) |&gt;\n  rename(cbsa = GEOID)\n\nx_ref &lt;- median(viz1_df$avg_permits_per_1000, na.rm = TRUE)\n\nggplot(viz1_df, aes(x = avg_permits_per_1000, y = rb_change)) +\n  geom_hline(yintercept = 0, linewidth = 0.4, linetype = \"dashed\") +\n  geom_vline(xintercept = x_ref, linewidth = 0.4, linetype = \"dashed\") +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Housing Growth vs Change in Rent Burden\",\n    subtitle = \"Right = more permits per 1,000 residents (2014–2023). Below zero = rent burden fell (2021–2023 vs 2009–2011).\",\n    x = \"Avg permits per 1,000 residents (2014–2023)\",\n    y = \"Change in rent burden share (late minus early)\"\n  ) +\n  scale_x_continuous(labels = label_number(accuracy = 0.1)) +\n  scale_y_continuous(labels = percent_format(accuracy = 0.1)) +\n  theme_minimal(base_size = 12)\n\n\n\n\n\n\n\n\n\n\n\nHousing Growth vs Population Growth\n\n\nShow code\npop_growth &lt;- POPULATION |&gt;\n  filter(year %in% c(2009, 2023)) |&gt;\n  select(GEOID, NAME, year, population) |&gt;\n  tidyr::pivot_wider(names_from = year, values_from = population, names_prefix = \"y_\") |&gt;\n  mutate(pop_growth_rate = (y_2023 - y_2009) / y_2009) |&gt;\n  filter(is.finite(pop_growth_rate))\n\nviz2_df &lt;- pop_growth |&gt;\n  inner_join(permits_per_1k, join_by(GEOID == CBSA))\n\nx_ref2 &lt;- median(viz2_df$pop_growth_rate, na.rm = TRUE)\ny_ref2 &lt;- median(viz2_df$avg_permits_per_1000, na.rm = TRUE)\n\nggplot(viz2_df, aes(x = pop_growth_rate, y = avg_permits_per_1000)) +\n  geom_vline(xintercept = x_ref2, linewidth = 0.4, linetype = \"dashed\") +\n  geom_hline(yintercept = y_ref2, linewidth = 0.4, linetype = \"dashed\") +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Population Growth vs Housing Permits\",\n    subtitle = \"Upper-right = growing population and high permitting\",\n    x = \"Population growth rate (2009–2023)\",\n    y = \"Avg permits per 1,000 residents (2014–2023)\"\n  ) +\n  scale_x_continuous(labels = percent_format(accuracy = 0.1)) +\n  scale_y_continuous(labels = label_number(accuracy = 0.1)) +\n  theme_minimal(base_size = 12)"
  },
  {
    "objectID": "mp02.html#policy-brief",
    "href": "mp02.html#policy-brief",
    "title": "Mini-Project #2 - Making Backyards Affordable for All",
    "section": "Policy Brief",
    "text": "Policy Brief\n\nWhy Housing Affordability Is a Policy Choice\nHousing affordability is not a mystery. It rises and falls based on whether a city builds enough homes for the people who want to live there. When housing supply keeps up with demand, rent stays manageable. When supply is restricted, rent climbs, families leave, and the middle class disappears. Fifteen years of Census and BLS data make this pattern impossible to ignore. Cities that build stay livable. Cities that block development become unaffordable.\nHouston is a clear example of what happens when a region allows housing to be built at scale. From 2010 to 2023, the metro permitted enough homes to keep rent burden around 22 percent of household income, even while its population grew rapidly. New York followed the opposite path. It permitted too little housing relative to demand, rent pressure remained high, and the average household continued spending more than 30 percent of income just to stay housed. The difference is not culture or luck. It is policy.\n\n\nWho Is Harmed and Who Benefits\nThe people most affected by high rent are not developers or investors. They are teachers, nurses, city workers, and other essential employees who keep a city functioning. In places like New York, these workers spend a third or more of their income on housing, which forces many to commute long distances or leave entirely. In Houston, the same occupations can afford to live closer to where they work, because housing supply was allowed to grow instead of freeze.\nHigh-rent cities are not preserving communities. They are pushing them out. Schools lose young teachers. Hospitals lose trained staff. Cities lose the very taxpayers they depend on. Meanwhile, cities that build enough housing keep their workforce, keep their culture, and keep their economic future.\n\n\nThe Federal YIMBY Incentive Proposal\nThe goal of the proposed federal YIMBY incentive bill is to steer federal support toward cities that make housing affordable through action, not slogans. Instead of a single national mandate, the program ties federal grants and infrastructure funding to measurable outcomes. Cities that reduce rent burden and permit enough new homes to match population growth move to the front of the line for federal support.\nProgress is tracked with two simple metrics:\n\nRent Burden Score measures how much of the average household’s income goes to rent.\nHousing Growth Score compares new homes built to both population size and population change.\n\nThese indicators make it easy to see which cities are becoming more affordable and which are not. A strong sponsorship team could pair a representative from Houston, which already shows how pro-housing policy works, with a representative from New York, where affordability is still collapsing and reform is overdue. The message is simple: building more housing is not an experiment. We already know it works.\n\n\nExtra Credit #03: Millennial Retention Clause\nThe bill also rewards cities that successfully retain residents between ages 25 and 34. This group is the future workforce, the future tax base, and the group most likely to leave when rent gets too high. Cities that build housing tend to keep young people. Cities that restrict supply lose them, along with the jobs, culture, and energy they bring. Housing policy is not just about rent. It is about whether the next generation sees a reason to stay."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Hi everyone, I’m Christopher Tan! I began my undergrad planning to pursue a career in accounting, but towards the end of my senior year, I took a handful of programming classes and discovered how much I enjoyed problem solving. That led me to pursuing my Masters in Statistics, where I could learn to make sense of large amounts of data as well as understanding statistical methodologies. I’m especially interested in ML and data science, since they bring together coding and statistical analysis in meaningful ways. Looking forward to meeting you all!"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "",
    "section": "",
    "text": "Hi everyone, I’m Christopher Tan! I began my undergrad planning to pursue a career in accounting, but towards the end of my senior year, I took a handful of programming classes and discovered how much I enjoyed problem solving. That led me to pursuing my Masters in Statistics, where I could learn to make sense of large amounts of data as well as understanding statistical methodologies. I’m especially interested in ML and data science, since they bring together coding and statistical analysis in meaningful ways. Looking forward to meeting you all!"
  },
  {
    "objectID": "mp01.html",
    "href": "mp01.html",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "",
    "text": "For this assignment, we are given the task of analyzing Netflix’s Global Top 10 and Country Top 10 datasets. Below we will acquire, clean, and explore the data, then answer exploratory questions and draft press releases."
  },
  {
    "objectID": "mp01.html#objective",
    "href": "mp01.html#objective",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "",
    "text": "For this assignment, we are given the task of analyzing Netflix’s Global Top 10 and Country Top 10 datasets. Below we will acquire, clean, and explore the data, then answer exploratory questions and draft press releases."
  },
  {
    "objectID": "mp01.html#acquire-data",
    "href": "mp01.html#acquire-data",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Acquire Data",
    "text": "Acquire Data\n\n\nShow code\nif (!dir.exists(file.path(\"data\", \"mp01\"))) {\n  dir.create(file.path(\"data\", \"mp01\"), showWarnings = FALSE, recursive = TRUE)\n}\n\nGLOBAL_TOP_10_FILENAME  &lt;- file.path(\"data\", \"mp01\", \"global_top10_alltime.csv\")\nCOUNTRY_TOP_10_FILENAME &lt;- file.path(\"data\", \"mp01\", \"country_top10_alltime.csv\")\n\nif (!file.exists(GLOBAL_TOP_10_FILENAME)) {\n  download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-global.tsv\",\n                destfile = GLOBAL_TOP_10_FILENAME)\n}\nif (!file.exists(COUNTRY_TOP_10_FILENAME)) {\n  download.file(\"https://www.netflix.com/tudum/top10/data/all-weeks-countries.tsv\",\n                destfile = COUNTRY_TOP_10_FILENAME)\n}"
  },
  {
    "objectID": "mp01.html#data-import",
    "href": "mp01.html#data-import",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Data Import",
    "text": "Data Import\n\n\nShow code\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(readr)\nlibrary(dplyr)\nlibrary(stringr)\nlibrary(DT)\n\n\n\n\nShow code\nGLOBAL_TOP_10 &lt;- read_tsv(GLOBAL_TOP_10_FILENAME)\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME)"
  },
  {
    "objectID": "mp01.html#data-cleaningreformatting",
    "href": "mp01.html#data-cleaningreformatting",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Data Cleaning/Reformatting",
    "text": "Data Cleaning/Reformatting\n\n\nShow code\nGLOBAL_TOP_10 &lt;- GLOBAL_TOP_10 |&gt;\nmutate(season_title = if_else(season_title == \"N/A\", NA_character_, season_title))\n\n\n\n\nShow code\nCOUNTRY_TOP_10 &lt;- read_tsv(COUNTRY_TOP_10_FILENAME,\n                          na = \"N/A\")"
  },
  {
    "objectID": "mp01.html#intial-data-exploration",
    "href": "mp01.html#intial-data-exploration",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Intial Data Exploration",
    "text": "Intial Data Exploration\n\n\nShow code\nlibrary(DT)\nGLOBAL_TOP_10 |&gt; \n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE))\n\n\n\n\n\n\n\n\nShow code\nlibrary(stringr)\nformat_titles &lt;- function(df){\n    colnames(df) &lt;- str_replace_all(colnames(df), \"_\", \" \") |&gt; str_to_title()\n    df\n}\n\nGLOBAL_TOP_10 |&gt; \n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\nShow code\nGLOBAL_TOP_10 |&gt; \n    select(-season_title) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))\n\n\n\n\n\n\n\n\nShow code\nGLOBAL_TOP_10 |&gt; \n    mutate(`runtime_(minutes)` = round(60 * runtime)) |&gt;\n    select(-season_title, \n           -runtime) |&gt;\n    format_titles() |&gt;\n    head(n=20) |&gt;\n    datatable(options=list(searching=FALSE, info=FALSE)) |&gt;\n    formatRound(c('Weekly Hours Viewed', 'Weekly Views'))"
  },
  {
    "objectID": "mp01.html#exploratory-questions",
    "href": "mp01.html#exploratory-questions",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Exploratory Questions",
    "text": "Exploratory Questions\n\n1. How many different countries does Netflix operate in?\n\n\nShow code\ncountries &lt;- COUNTRY_TOP_10 |&gt; \n  summarise(num_countries = n_distinct(country_name))\n\n\nNetflix operates in 94 countries.\n\n\n2. Which non-English-language film has spent the most cumulative weeks in the global top 10? How many weeks did it spend?\n\n\nShow code\nnonEnglish &lt;- GLOBAL_TOP_10 |&gt; \n  filter(category == \"Films (Non-English)\") |&gt;\n  arrange(desc(cumulative_weeks_in_top_10)) |&gt;\n  slice(1) |&gt;\n  select(show_title, cumulative_weeks_in_top_10)\n\n\nThe non-English film with the most cumulative weeks in the global Top 10 is All Quiet on the Western Front, with 23 weeks.\n\n\n3. What is the longest film (English or non-English) to have ever appeared in the Netflix global Top 10? How long is it in minutes?\n\n\nShow code\nlongest &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(category, \"Films\")) |&gt;\n  mutate(runtime_minutes = runtime * 60) |&gt;\n  arrange(desc(runtime_minutes)) |&gt; \n  slice(1) |&gt;\n  select(show_title, runtime_minutes)\n\n\nThe longest film to have ever appeared in the Netflix global Top 10 was Pushpa 2: The Rule (Reloaded Version) with 224 minutes.\n\n\n4. For each of the four categories, what program has the most total hours of global viewership?\n\n\nShow code\ntop_by_category &lt;- GLOBAL_TOP_10 |&gt;\n  group_by(category, show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop_last\") |&gt;\n  slice_max(order_by = total_hours, n = 1, with_ties = FALSE) |&gt;\n  ungroup()\n\n\nThe programs with the most total hours of global viewership are KPop Demon Hunters for English films, Society of the Snow for non-English films, Stranger Things for English TV, and Squid Game for non-English TV.\n\n\n5. Which TV show had the longest run in a country’s Top 10? How long was this run and in what country did it occur?\n\n\nShow code\nlongest_tv_run &lt;- COUNTRY_TOP_10 |&gt;\n  filter(category == \"TV\") |&gt;\n  group_by(country_name, show_title) |&gt;\n  summarise(longest_run = max(cumulative_weeks_in_top_10, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(longest_run)) |&gt;\n  slice(1)   \n\n\nThe TV show that had the longest run in a country’s Top 10 is Money Heist, which appeared in Pakistan for 127 weeks.\n\n\n6. Netflix provides over 200 weeks of service history for all but one country in our data set. Which country is this and when did Netflix cease operations in that country?\n\n\nShow code\ndropped_country &lt;- COUNTRY_TOP_10 |&gt;\n  group_by(country_name) |&gt;\n  summarise(max_week = max(week, na.rm = TRUE)) |&gt;\n  arrange(max_week) |&gt;\n  slice(1)\n\n\nThe country where Netflix ceased operations the earliest is Russia, in the week of 2022-02-27.\n\n\n7. What is the total viewership of the TV show Squid Game? Note that there are three seasons total and we are looking for the total number of hours watched across all seasons.\n\n\nShow code\nsquid_game_viewership &lt;- GLOBAL_TOP_10 |&gt;\n  filter(str_detect(show_title, \"Squid Game\")) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE))\n\n\nThe total viewership of the TV show Squid Game across all seasons is 5,310,000,000 hours.\n\n\n8. The movie Red Notice has a runtime of 1 hour and 58 minutes. Approximately how many views did it receive in 2021?\n\n\nShow code\nred_notice_views_2021 &lt;- GLOBAL_TOP_10 |&gt;\n  filter(show_title == \"Red Notice\", year(week) == 2021) |&gt;\n  summarise(total_views = sum(weekly_hours_viewed, na.rm = TRUE) / ((1 + 58 / 60))) |&gt;\n  mutate(total_views = round(total_views))\n\n\nThe movie Red Notice received approximately 201,732,203 views in 2021.\n\n\n9. How many Films reached Number 1 in the US but did not originally debut there?\n\n\nShow code\nlibrary(dplyr)\nlibrary(stringr)\n\nus_films &lt;- COUNTRY_TOP_10 |&gt;\n  filter(country_name == \"United States\",\n         str_starts(category, \"Films\")) |&gt;\n  group_by(show_title) |&gt;\n  summarise(\n    debut_week = min(week, na.rm = TRUE),\n    debut_rank = weekly_rank[which.min(week)],\n    hit_one = any(weekly_rank == 1, na.rm = TRUE),\n    last_week_one = if (any(weekly_rank == 1, na.rm = TRUE))\n      max(week[weekly_rank == 1], na.rm = TRUE)\n    else as.Date(NA),\n    .groups = \"drop\"\n  )\n\nclimbers &lt;- us_films |&gt;\n  filter(hit_one, debut_rank &gt; 1) |&gt;\n  arrange(desc(last_week_one))\n\nfilms_count &lt;- nrow(climbers)\nlatest_row  &lt;- slice_head(climbers, n = 1)\nlatest_film &lt;- latest_row$show_title\nlatest_when &lt;- latest_row$last_week_one\n\n\nA total of 45 films reached Number 1 in the United States after debuting below the top spot.\nThe most recent film to do this was KPop Demon Hunters, which reached Number 1 on 2025-09-14.\n\n\n10. Which TV show/season hit the top 10 in the most countries in its debut week? In how many countries did it chart?\n\n\nShow code\nlibrary(dplyr)\nlibrary(stringr)\n\ntv_debut_spread &lt;- COUNTRY_TOP_10 |&gt;\n  filter(str_starts(category, \"TV\")) |&gt;\n  group_by(show_title, season_title) |&gt;\n  mutate(debut_week = min(week, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  filter(week == debut_week) |&gt;\n  group_by(show_title, season_title) |&gt;\n  summarise(\n    countries = n_distinct(country_name),\n    debut_week = first(debut_week),\n    .groups = \"drop\"\n  ) |&gt;\n  slice_max(countries, n = 1, with_ties = FALSE)\n\ntop_title     &lt;- tv_debut_spread$show_title\ntop_season    &lt;- tv_debut_spread$season_title\ntop_countries &lt;- tv_debut_spread$countries\ntop_week      &lt;- tv_debut_spread$debut_week\n\n\nThe TV show/season that hit the Top 10 in the most countries in its debut week was Emily in Paris — Emily in Paris: Season 2, charting in 94 countries during the week of 2021-12-26."
  },
  {
    "objectID": "mp01.html#press-releases",
    "href": "mp01.html#press-releases",
    "title": "Mini-Project #1 - Netflix Top 10",
    "section": "Press Releases",
    "text": "Press Releases\n\n\nShow code\n# Global table uses \"TV (English)\" for the category.\nst_global &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (English)\",\n         str_detect(show_title, regex(\"^Stranger Things$\", ignore_case = TRUE)))\n\nst_total_hours &lt;- sum(st_global$weekly_hours_viewed, na.rm = TRUE)  # FACT 1\nst_total_weeks &lt;- nrow(st_global)                                   # FACT 2\n\n# Country table uses \"TV\" (no language tag). Count unique countries.\nst_countries &lt;- COUNTRY_TOP_10 |&gt;\n  filter(category == \"TV\",\n         str_detect(show_title, regex(\"^Stranger Things$\", ignore_case = TRUE)))\n\nst_num_countries &lt;- st_countries |&gt;\n  distinct(country_name) |&gt;\n  nrow()                                                            # FACT 3\n\n# Compare against other English-language TV series by total hours viewed\ntv_eng_totals &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category == \"TV (English)\") |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(total_hours))\n\nst_rank &lt;- tv_eng_totals |&gt;\n  mutate(rank = dense_rank(desc(total_hours))) |&gt;\n  filter(show_title == \"Stranger Things\") |&gt;\n  pull(rank)\n\nsecond_place  &lt;- tv_eng_totals |&gt; slice(2)\nsecond_show   &lt;- second_place$show_title\nsecond_hours  &lt;- second_place$total_hours\n\n\n\nStranger Things Season 5: The Resurgence of a Legacy\nNetflix’s Stranger Things remains a once-in-a-generation hit as many await its final release in November 2025. Across its first four seasons, the series has amassed 2,967,980,000 hours viewed globally, spent 50 weeks in the Global Top 10, and charted in 93 countries. Among English-language TV series by total hours viewed, it ranks #1, ahead of hits like Wednesday (2,842,150,000 hours), underscoring extraordinary staying power and worldwide appeal ahead of the final season.\n\n\nShow code\n# Identify Hindi-language titles \nhindi_global &lt;- GLOBAL_TOP_10 |&gt;\n  filter(category %in% c(\"Films (Non-English)\", \"TV (Non-English)\"),\n         str_detect(show_title, regex(\"Hindi|India\", ignore_case = TRUE)))\n\n# Aggregate total hours viewed globally \nhindi_total_hours &lt;- sum(hindi_global$weekly_hours_viewed, na.rm = TRUE)\n\n# Find top Hindi titles by global hours viewed\ntop_hindi &lt;- hindi_global |&gt;\n  group_by(show_title) |&gt;\n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(total_hours)) |&gt;\n  slice_head(n = 3)\n\ntop_hindi_1  &lt;- top_hindi$show_title[1]\ntop_hindi_1h &lt;- top_hindi$total_hours[1]\ntop_hindi_2  &lt;- top_hindi$show_title[2]\ntop_hindi_2h &lt;- top_hindi$total_hours[2]\ntop_hindi_3  &lt;- top_hindi$show_title[3]\ntop_hindi_3h &lt;- top_hindi$total_hours[3]\n\n# Estimate the number of unique Hindi titles appearing in the dataset\nhindi_title_count &lt;- n_distinct(hindi_global$show_title)\n\n# Estimate number of Hindi entries (viewing records)\nhindi_records &lt;- nrow(hindi_global)\n\n\n\n\nNetflix Celebrates Blockbuster Growth in India\nIndia’s love for local storytelling continues to fuel Netflix’s remarkable rise in South Asia. Over the past two years, Hindi-language films and series have generated more than 244,100,000 hours viewed worldwide. Among these titles, hits like RRR (Hindi) (79,780,000 hours), The Great Indian Kapil Show (44,600,000 hours), and Kalki 2898 AD (Hindi) (23,100,000 hours) have captivated audiences at home as well as overseas. With over 20 Hindi-language titles appearing in the global Top 10 across 60 charting weeks, Netflix continues to deepen its investment in local content — cementing India as one of its fastest-growing creative and consumer markets.\n\n\nShow code\njp_global &lt;- GLOBAL_TOP_10 |&gt; \n  filter(category %in% c(\"Films (Non-English)\", \"TV (Non-English)\")) |&gt; \n  filter(str_detect(show_title, regex(\"Japan|Japanese|の|アニメ|ドラマ|Tokyo|Naruto|One Piece|Jujutsu Kaisen|Demon Slayer|Spirited Away|Ghibli|Boruto\", ignore_case = TRUE)))\n\n# Compute totals\njp_total_hours &lt;- sum(jp_global$weekly_hours_viewed, na.rm = TRUE)\njp_title_count &lt;- n_distinct(jp_global$show_title)\njp_records &lt;- nrow(jp_global)\n\n# Find top 3 Japanese titles by total hours viewed\ntop_jp &lt;- jp_global |&gt; \n  group_by(show_title) |&gt; \n  summarise(total_hours = sum(weekly_hours_viewed, na.rm = TRUE), .groups = \"drop\") |&gt; \n  arrange(desc(total_hours)) |&gt; \n  slice_head(n = 3)\n\ntop_jp_1  &lt;- top_jp$show_title[1]\ntop_jp_1h &lt;- top_jp$total_hours[1]\ntop_jp_2  &lt;- top_jp$show_title[2]\ntop_jp_2h &lt;- top_jp$total_hours[2]\ntop_jp_3  &lt;- top_jp$show_title[3]\ntop_jp_3h &lt;- top_jp$total_hours[3]\n\n\n\n\nThe Global Appeal of Japanese Storytelling\nJapanese films and series are finding fans in every corner of the world. From touching dramas to breathtaking anime, stories from Japan continue to draw in global audiences with their emotion, imagination, and artistry. Over the past few years, viewers have spent more than 178,970,000 hours watching Japanese titles on Netflix. Fan favorites like Demon Slayer: Kimetsu no Yaiba (89,900,000 hours), Tokyo Swindlers (41,900,000 hours), and Jujutsu Kaisen (22,400,000 hours) continue to reach new audiences and showcase the creativity of Japanese storytellers. With more than 8 Japanese titles appearing in the Top 10 across 30 charting weeks, Netflix’s collaboration with Japan’s creators is helping share their stories with millions of viewers around the world."
  },
  {
    "objectID": "mp03.html",
    "href": "mp03.html",
    "title": "Mini-Project #3 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "This mini-project works to analyze NYC’s street trees by combining council district boundaries with detailed tree data from NYC OpenData. The goal is to visualize where trees are located, assess their condition and species patterns, and use these findings to propose a targeted improvement plan for one district supported by data and maps."
  },
  {
    "objectID": "mp03.html#introduction",
    "href": "mp03.html#introduction",
    "title": "Mini-Project #3 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "",
    "text": "This mini-project works to analyze NYC’s street trees by combining council district boundaries with detailed tree data from NYC OpenData. The goal is to visualize where trees are located, assess their condition and species patterns, and use these findings to propose a targeted improvement plan for one district supported by data and maps."
  },
  {
    "objectID": "mp03.html#data-acquisition",
    "href": "mp03.html#data-acquisition",
    "title": "Mini-Project #3 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Data Acquisition",
    "text": "Data Acquisition\n\nTask 1: Download NYC City Council District Boundaries\n\n\nShow code\nlibrary(sf)\nlibrary(dplyr)\nlibrary(httr2)\nlibrary(ggplot2)\nlibrary(DT)\n\n# download from NYC City Council District website\ndownload.file(\nurl = \"https://s-media.nyc.gov/agencies/dcp/assets/files/zip/data-tools/bytes/city-council/nycc_25c.zip\", \ndest = \"data/mp03/nycc_25c.zip\", \nmode = \"wb\")\n\n#unzip in same directory\nunzip(\"data/mp03/nycc_25c.zip\", exdir = \"data/mp03\")\n\n#read shp file to DATA\nDATA &lt;- st_read(\"data/mp03/nycc_25c/nycc.shp\")\n\n\nReading layer `nycc' from data source \n  `C:\\Users\\chris\\OneDrive\\Documents\\STA9750-2025-FALL\\data\\mp03\\nycc_25c\\nycc.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 51 features and 3 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 913175.1 ymin: 120128.4 xmax: 1067383 ymax: 272844.3\nProjected CRS: NAD83 / New York Long Island (ftUS)\n\n\nShow code\n#transforms district boundaries to a more standard system\nDATA &lt;- st_transform(DATA, crs = \"WGS84\")\n\n\n\n\nTask 2: Download Tree Points\n\n\nShow code\ndownload_tree_points &lt;- function(\n  data_dir = \"data/mp03\",\n  base_url = \"https://data.cityofnewyork.us/resource/hn5i-inap.geojson\", # &lt;- GeoJSON API\n  limit    = 50000  # rows per page\n) {\n\n  # make sure directory exists\n  dir.create(data_dir, recursive = TRUE, showWarnings = FALSE)\n\n  page      &lt;- 0L\n  all_files &lt;- character()\n\n  repeat {\n    offset   &lt;- page * limit\n    file_out &lt;- file.path(\n      data_dir,\n      sprintf(\"tree_points_%06d.geojson\", offset)\n    )\n\n    # Only download if file not already present (polite usage)\n    if (!file.exists(file_out)) {\n      message(\"Downloading trees with offset = \", offset)\n\n      resp &lt;- request(base_url) |&gt;\n        req_url_query(\n          `$limit`  = limit,\n          `$offset` = offset\n        ) |&gt;\n        req_perform()\n\n      # Save raw GeoJSON to disk\n      writeBin(resp_body_raw(resp), file_out)\n    } else {\n      message(\"Skipping offset = \", offset, \" (file already exists)\")\n    }\n\n    # Read this page once to check how many rows we got\n    this_sf &lt;- st_read(file_out, quiet = TRUE)\n    n_rows  &lt;- nrow(this_sf)\n\n    if (n_rows == 0) {\n      message(\"No rows returned at offset = \", offset, \" — stopping.\")\n      file.remove(file_out)\n      break\n    }\n\n    all_files &lt;- c(all_files, file_out)\n\n    if (n_rows &lt; limit) {\n      message(\"Last page reached with \", n_rows, \" rows at offset = \", offset)\n      break\n    }\n\n    page &lt;- page + 1L\n  }\n\n  # Read all saved GeoJSON files and combine into one sf object\n  trees_list &lt;- lapply(all_files, st_read, quiet = TRUE)\n  trees_sf   &lt;- bind_rows(trees_list)\n\n  # Make sure CRS matches council districts (DATA from Task 1)\n  trees_sf &lt;- st_transform(trees_sf, st_crs(DATA))\n\n  trees_sf\n}\n\ntree_points &lt;- download_tree_points()"
  },
  {
    "objectID": "mp03.html#data-integration-and-initial-exploration",
    "href": "mp03.html#data-integration-and-initial-exploration",
    "title": "Mini-Project #3 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Data Integration and Initial Exploration",
    "text": "Data Integration and Initial Exploration\n\nTask 3: Plot All Tree Points\n\n\nShow code\nggplot() +\n  geom_sf(data = DATA, fill = NA, color = \"grey50\", linewidth = 0.2) +\n  geom_sf(data = tree_points, size = 0.1, alpha = 0.3, color = \"darkgreen\") +\n  coord_sf() +\n  labs(\n    title    = \"NYC Trees and City Council Districts\",\n    x = NULL, y = NULL\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nTask 4: District-Level Analysis of Tree Coverage\n\n1. Which council district has the most trees?\n\n\nShow code\ntrees_with_districts &lt;- st_join(\n  tree_points,\n  DATA,\n  join = st_intersects,\n  left = FALSE\n)\n\ntrees_with_districts |&gt;\n  st_drop_geometry() |&gt;\n  count(CounDist, name = \"n_trees\") |&gt;\n  arrange(desc(n_trees)) |&gt;\n  slice_head(n = 1) |&gt;\n  rename(`Council District` = CounDist,`Number of Trees`  = n_trees)\n\n\n  Council District Number of Trees\n1               51           70965\n\n\nThe council district that has the most trees is District 51.\n\n\n2. Which council district has the highest density of trees?\n\n\nShow code\n# 1. Get district areas\ndistrict_area &lt;- DATA |&gt;\n  st_drop_geometry() |&gt;\n  select(CounDist, Shape_Area)\n\n# 2. Count trees per district FIRST\ntrees_by_district &lt;- trees_with_districts |&gt;\n  st_drop_geometry() |&gt;\n  count(CounDist, name = \"n_trees\")\n\n# 3. Join counts + area → compute density\ntrees_by_district |&gt;\n  left_join(district_area, by = \"CounDist\") |&gt;\n  mutate(tree_density = n_trees / Shape_Area) |&gt;\n  arrange(desc(tree_density)) |&gt;\n  slice_head(n = 1) |&gt;\n  rename(\n    `Council District` = CounDist,\n    `Tree Density`     = tree_density,\n    `Area` = Shape_Area,\n    `Number of Trees` = n_trees\n  )\n\n\n  Council District Number of Trees     Area Tree Density\n1                7           15648 55186140 0.0002835495\n\n\nThe council district that has the highest density of trees is District 7.\n\n\n3. Which district has highest fraction of dead trees out of all trees?\n\n\nShow code\ntrees_with_districts |&gt;\n  st_drop_geometry() |&gt;\n  mutate(is_dead = tpcondition == \"Dead\") |&gt;\n  group_by(CounDist) |&gt;\n  summarise(\n    total_trees = n(),\n    dead_trees  = sum(is_dead, na.rm = TRUE),\n    frac_dead   = dead_trees / total_trees\n  ) |&gt;\n  arrange(desc(frac_dead)) |&gt;\n  slice_head(n = 1) |&gt;\n  rename(\n    `Council District` = CounDist,\n    `Fraction Dead`    = frac_dead,\n    `Dead Trees`       = dead_trees,\n    `Total Trees`      = total_trees\n  )\n\n\n# A tibble: 1 × 4\n  `Council District` `Total Trees` `Dead Trees` `Fraction Dead`\n               &lt;int&gt;         &lt;int&gt;        &lt;int&gt;           &lt;dbl&gt;\n1                 32         30270         4315           0.143\n\n\nThe district that has the highest fraction of dead trees out of all trees is District 32.\n\n\n4. What is the most common tree species in Manhattan?\n\n\nShow code\ntrees_with_borough &lt;- trees_with_districts |&gt;\n  mutate(\n    dist_num = as.integer(CounDist),\n    borough = case_when(\n      dist_num &gt;=  1 & dist_num &lt;= 10 ~ \"Manhattan\",\n      dist_num &gt;= 11 & dist_num &lt;= 18 ~ \"Bronx\",\n      dist_num &gt;= 19 & dist_num &lt;= 32 ~ \"Queens\",\n      dist_num &gt;= 33 & dist_num &lt;= 48 ~ \"Brooklyn\",\n      dist_num &gt;= 49 & dist_num &lt;= 51 ~ \"Staten Island\"\n    )\n  )\n\ntrees_with_borough |&gt;\n  filter(borough == \"Manhattan\") |&gt;\n  st_drop_geometry() |&gt;\n  count(genusspecies, sort = TRUE) |&gt;\n  slice_head(n = 1) |&gt;\n  rename(\n    `Most Common Species (Manhattan)` = genusspecies,\n    `Number of Trees`                 = n\n  )\n\n\n                             Most Common Species (Manhattan) Number of Trees\n1 Gleditsia triacanthos var. inermis - Thornless honeylocust           17310\n\n\nThe most common tree species in Manhattan is Gleditsia triacanthos var. inermis - Thornless honeylocust.\n\n\n5. What is the species of the tree closest to Baruch’s campus?\n\n\nShow code\nlibrary(sf)\n\nnew_st_point &lt;- function(lat, lon) {\n  st_sfc(st_point(c(lon, lat)), crs = \"WGS84\")\n}\n\nbaruch &lt;- new_st_point(lat = 40.7403, lon = -73.9833)\n\ntrees_with_borough |&gt;\n  filter(borough == \"Manhattan\") |&gt; \n  select(geometry, genusspecies) |&gt;\n  mutate(distance = st_distance(geometry, baruch)) |&gt;\n  arrange(distance) |&gt;\n  slice(1) |&gt;\n  pull(genusspecies)\n\n\n[1] \"Liquidambar styraciflua - sweetgum\"\n\n\nThe species of the tree closest to Baruch’s campus is “Liquidambar styraciflua - sweetgum”."
  },
  {
    "objectID": "mp03.html#task-5-nyc-parks-proposal",
    "href": "mp03.html#task-5-nyc-parks-proposal",
    "title": "Mini-Project #3 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Task 5: NYC Parks Proposal",
    "text": "Task 5: NYC Parks Proposal\n\nOverview\nDistrict 32 has a substantial amount of dead/unhealthy trees based on the 2024 Tree Census. Many of these trees run along many of the older residential blocks that get a lot of foot traffic but do not have much shade. Thus, this initiative will focus on removing unsafe trees and planting more resilient species.\n\n\nQuantitative Statement\n\n\n\nAction\nQuantity\n\n\n\n\nRemoving dead trees\nAbout 400\n\n\nNew replacement plants\nAround 500\n\n\n\n\n\nTrees in District 16\n\n\nShow code\ndistrict_16 &lt;- DATA |&gt;\ndplyr::filter(CounDist == \"16\")\n\ntrees_dist16 &lt;- trees_with_districts |&gt;\ndplyr::filter(CounDist == \"16\")\n\nbbox16 &lt;- sf::st_bbox(district_16)\n\nggplot() +\n  geom_sf(data = district_16,\n          fill = \"grey95\",\n          color = \"black\",\n          linewidth = 0.4) +\n  geom_sf(data = trees_dist16,\n          size  = 0.3,\n          alpha = 0.5,\n          color = \"darkgreen\") +\n  coord_sf(\n    xlim = c(bbox16[\"xmin\"], bbox16[\"xmax\"]),\n    ylim = c(bbox16[\"ymin\"], bbox16[\"ymax\"])\n  ) +\n  labs(\n    title = \"Street Trees in District 16\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme_minimal() +\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    plot.title  = element_text(hjust = 0.5)  # &lt;-- Center the title\n  )\n\n\n\n\n\n\n\n\n\n\n\nQuantitative Comparison\nDistrict 16 has the highest dead-tree ratio among the four districts compared, with roughly 13% of its trees classified as dead or unhealthy. This is higher than District 14 (about 12%), District 3 (around 11%), and far above District 41 (about 7%). Even though these districts have similar overall tree counts, District 16 stands out for having the worst canopy health and the largest share of failing trees. This makes District 16 the district most in need of targeted removal and replacement efforts, supporting the justification for directing additional Parks Department resources toward improving tree conditions in this area.\n\n\nComparison Support\nTo further support this comparison, I’ve constructed a bar chart comparing the dead tree ratios between districts 16, 3, 14, and 41.\n\n\nShow code\nlibrary(scales)\ncompare_ids &lt;- c(16, 14, 3, 41) #districts for further comparison\n\ndead_comp &lt;- trees_with_districts |&gt;\nst_drop_geometry() |&gt;\nfilter(CounDist %in% compare_ids) |&gt;\nmutate(is_dead = tpcondition == \"Dead\") |&gt;\ngroup_by(CounDist) |&gt;\nsummarise(\ntotal_trees = n(),\ndead_trees = sum(is_dead, na.rm = TRUE),\ndead_ratio = dead_trees / total_trees\n)\n\nggplot(dead_comp, aes(x = factor(CounDist), y = dead_ratio, fill = factor(CounDist))) +\n  geom_col(width = 0.9, alpha = 2) +\n  scale_y_continuous(\n    labels = percent_format(),\n    expand = expansion(mult = c(0, 0.15))\n  ) +\n  labs(\n    title = \"Dead Tree Ratio by Council District\",\n    x = \"Council District\",\n    y = \"Percent Dead Trees\"\n  ) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title      = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nDistrict 16 vs. District 14\n\n\nShow code\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(sf)\nlibrary(patchwork)  # for p16 + p14 layout\n\n# helper to plot dead trees in a single district\nplot_dead_trees &lt;- function(district_num) {\n  district &lt;- DATA |&gt;\n    filter(CounDist == district_num)\n\n  trees_dead &lt;- trees_with_districts |&gt;\n    filter(CounDist == district_num, tpcondition == \"Dead\")\n\n  ggplot() +\n    geom_sf(data = district,\n            fill = \"grey95\",\n            color = \"black\",\n            linewidth = 0.4) +\n    geom_sf(data = trees_dead,\n            size  = 0.5,\n            alpha = 0.6,\n            color = \"red\") +\n    coord_sf() +\n    labs(\n      title = paste(\"Dead Trees in District\", district_num)\n    ) +\n    theme_void() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n}\n\np16 &lt;- plot_dead_trees(16)\np14 &lt;- plot_dead_trees(14)\n\np16 + p14  # side-by-side comparison\n\n\n\n\n\n\n\n\n\nThe side-by-side maps show that District 16 has a much higher concentration of dead trees compared to District 14. Dead trees in District 16 appear in clear clusters across many residential blocks, while District 14 has fewer and more spread-out problem trees. This visual difference supports focusing removal and replacement efforts in District 16, since its higher number of failing trees impacts safety, shade, and overall canopy health."
  },
  {
    "objectID": "mp03.html#extra-credit",
    "href": "mp03.html#extra-credit",
    "title": "Mini-Project #3 - Visualizing and Maintaining the Green Canopy of NYC",
    "section": "Extra Credit",
    "text": "Extra Credit\n\nEC #01: Improved Tree Map Visualizations\nFor improved visualization and legibility, I have segmented the map into their respective districts, allowing each one to be toggled on or off.\n\n\nShow code\nlibrary(leaflet)\nlibrary(sf)\nlibrary(dplyr)\n\n# 1. Use trees_with_districts, not tree_points\ntrees_all &lt;- trees_with_districts |&gt;\n  st_transform(4326) |&gt;\n  mutate(\n    lon = st_coordinates(geometry)[, 1],\n    lat = st_coordinates(geometry)[, 2]\n  )\n\nDATA_wgs &lt;- DATA |&gt;\n  st_transform(4326)\n\n# 2. Get the list of districts that actually have trees\ndistrict_ids &lt;- sort(unique(trees_all$CounDist))\n\n# 3. Base map with district boundaries\nm &lt;- leaflet(options = leafletOptions(preferCanvas = TRUE)) |&gt;\n  addProviderTiles(providers$CartoDB.Positron) |&gt;\n  addPolygons(\n    data   = DATA_wgs,\n    fillColor = \"transparent\",\n    color     = \"black\",\n    weight    = 1,\n    group     = \"District Boundaries\",\n    label     = ~paste(\"District\", CounDist)\n  )\n\n# 4. Add a layer per district\nfor (d in district_ids) {\n  td &lt;- trees_all |&gt; filter(CounDist == d)\n  if (nrow(td) == 0L) next  # skip if no trees\n\n  m &lt;- m |&gt;\n    addCircleMarkers(\n      data        = td,\n      lng         = ~lon,\n      lat         = ~lat,\n      radius      = 1,\n      stroke      = FALSE,\n      fillColor   = \"darkgreen\",\n      fillOpacity = 0.4,\n      popup       = ~paste0(\"District: \", CounDist,\n                            \"&lt;br&gt;Species: \", genusspecies),\n      group       = paste(\"District\", d)\n    )\n}\n\n# 5. Layer control to toggle districts\nm &lt;- m |&gt;\n  addLayersControl(\n    overlayGroups = c(\"District Boundaries\",\n                      paste(\"District\", district_ids)),\n    options = layersControlOptions(collapsed = FALSE)\n  )"
  }
]